{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [20 pts] Define each of the following machine learning terms:\n",
    "\n",
    "* *dataset* - A collection of instances is a dataset and when working with machine learning methods we typically need a few datasets for different purposes.\n",
    "  * *instance* - a single row of data is called an instance. It is an observation from the domain.\n",
    "\n",
    "* *training* - a dataset that we feed into our machine learning algorithm to train our model\n",
    "\n",
    "* *testing* - a dataset that we use to validate the accuracy of our model but is not used to train the model. It may be called the validation dataset.\n",
    "* *validation dataset* - used to tune the models hyper-parameters.\n",
    "\n",
    "* *ground truth* - the value measured for your target variable for the training and testing examples where nearly all the time you can safely treat this the same as the label. If you augment your data set, there is a subtle difference between the ground truth (your actual measurements) and how the augmented examples relate to the labels you have assigned. \n",
    "\n",
    "* *label* - a human or machine generated description tagged to one or many data samples.\n",
    "\n",
    "* *pre-processing* - data transforming or encoding to bring data to a state such that a machine can easily parse it.\n",
    "\n",
    "* *feature* - an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.\n",
    "\n",
    "* *numerical* - numerical or quantitative data will always be a number that can be measured.\n",
    "\n",
    "* *nominal* - nominal data is classified without a natural rank, whereas ordinal data has a predetermined or natural order.\n",
    "\n",
    "* *decision surface* - A hyper surface in a multidimensional state space that partitions the space into  regions. Data lying on one side of a decision surface are defined as belonging to a different class form those lying on the the other. Decision surfaces may be created or modified as a result of a learning process and they are frequently used in machine learning, pattern recognition, and classification systems\n",
    "\n",
    "* *model validation* - the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.\n",
    "\n",
    "* *accuracy* - is a weighted arithmetic mean of precision and inverse precision (weighted by bias) as well as weighted arithmetic mean of recall and inverse recall (weighted by prevalence). Inverse precision and inverse recall are simply the precision and recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as ROC curves and provide a principled mechanism to explore operating point trade offs.\n",
    "\n",
    "* *cross-validation* - is a technique for evaluating ML models by training several ML models on a subset of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect over fitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [20 pts] Pick **two** of the [Scikit-learn datasets] which are already included in the library (i.e. the ones with datasets_load_) an find out the following:\n",
    "* the number of data points\n",
    "* the number of features and their types\n",
    "* the number and name of categories (i.e. the target field)\n",
    "* the mean (or mode if nominal) of the first two features\n",
    "\n",
    "[Scikit-learn datasets]: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine(as_frame=True)\n",
    "print(colored('number of data points:', 'green', attrs=['bold']))\n",
    "print(data.frame.count())\n",
    "print(colored('number of features:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes.size)\n",
    "print(colored('type of feature:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes)\n",
    "print(colored('number of categories:', 'green', attrs=['bold']))\n",
    "print(data.target_names.size)\n",
    "print(colored('name of categories: ', 'green', attrs=['bold']))\n",
    "print(data.target_names)\n",
    "print(colored('mean of alcohol: ', 'green', attrs=['bold']))\n",
    "print(data.frame['alcohol'].mean)\n",
    "print(colored('mean of malic_acid: ', 'green', attrs=['bold']))\n",
    "print(data.frame['malic_acid'].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing(as_frame=True)\n",
    "print(colored('number of data points:', 'green', attrs=['bold']))\n",
    "print(data.frame.count())\n",
    "print(colored('number of features:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes.size)\n",
    "print(colored('type of feature:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes)\n",
    "print(colored('number of categories:', 'green', attrs=['bold']))\n",
    "print(data.target.size)\n",
    "print(colored('name of categories: ', 'green', attrs=['bold']))\n",
    "print(data.target_names)\n",
    "print(colored('mean of MedInc: ', 'green', attrs=['bold']))\n",
    "print(data.frame['MedInc'].mean)\n",
    "print(colored('mean of HouseAge: ', 'green', attrs=['bold']))\n",
    "print(data.frame['HouseAge'].mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [40 pts] Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file (not provided, you have to download it by yourself by following the instructions in the module Jupyter notebook). Display the correlation matrix where each row and column are the features, which should be an 8 by 8 matrix (should we use 'Serial no'?). You can use pandas DataFrame.corr() to verify correctness of yours.\n",
    "\n",
    "Observe that the diagonal of this matrix should have all 1's and explain why? Since the last column can be used as the target (dependent) varaible, what do you think about the correlations between all the variables? Which variable should be the most important for prediction of 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/Admission_Predict_Ver1.1.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def correlationCoefficient(X, Y, n) : \n",
    "    sum_X = 0\n",
    "    sum_Y = 0\n",
    "    sum_XY = 0\n",
    "    squareSum_X = 0\n",
    "    squareSum_Y = 0 \n",
    "    i = 0\n",
    "    while i < n : \n",
    "        sum_X = sum_X + X[i] # sum of elements of array X. \n",
    "        sum_Y = sum_Y + Y[i] # sum of elements of array Y. \n",
    "        sum_XY = sum_XY + X[i] * Y[i] # sum of X[i] * Y[i]. \n",
    "        squareSum_X = squareSum_X + X[i] * X[i] # sum of square of array elements. \n",
    "        squareSum_Y = squareSum_Y + Y[i] * Y[i] \n",
    "        i = i + 1 \n",
    "    #Pearson correlation coefficient.\n",
    "    corr = (float)(n * sum_XY - sum_X * sum_Y)/ \\\n",
    "           (float)(math.sqrt((n * squareSum_X - \\\n",
    "           sum_X * sum_X)* (n * squareSum_Y - \\\n",
    "           sum_Y * sum_Y))) \n",
    "    return corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:.6f}'.format(correlationCoefficient(df['GRE Score'].to_numpy(),\n",
    "                             df['Chance of Admit '].to_numpy(),\n",
    "                             df['GRE Score'].size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in df.columns:\n",
    "    for Y in df.columns:\n",
    "        print('{0:.6f}'.format(correlationCoefficient(df[X].to_numpy(),df[Y].to_numpy(),df[X].size)),end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGPA has the highest correlation with Chance of Admit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. [20 pts] Classification of mushrooms, edible or poisonous. Download the *assignment01_mushroom_dataset.csv* dataset file from the module content. Load the data set in your model development framework, examine the features to see they are all nominal features. The first column is the class which represents whether the mushroom is poisonous or not. Apply necessary pre-processing such as nominal to numerical conversions. Make sure to sanity check the pipeline and perhaps run your favorite baseline classifier first.\n",
    "\n",
    "Report the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/assignment01_mushroom_dataset.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.columns.tolist()\n",
    "l.remove('class')\n",
    "df_o = pd.get_dummies(df, columns=l)\n",
    "df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_o['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "# We will reuse the classifier function below\n",
    "def rf_train_test(_X_tr, _X_ts, _y_tr, _y_ts):\n",
    "    # Create a new random forest classifier, with working 4 parallel cores\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=None, n_jobs=4)\n",
    "    # Train on training data\n",
    "    model = rf.fit(_X_tr, _y_tr)\n",
    "    # Test on training data\n",
    "    y_pred = rf.predict(_X_ts)\n",
    "    # Return accuracy\n",
    "    return accuracy_score(_y_ts, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input X matrix and target y vector\n",
    "X = df_o.loc[:, df_o.columns != 'class'].values\n",
    "y = df_o.loc[:, df_o.columns == 'class'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
    "rf_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run 100 times and collect statistics\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
    "    accuracies += [rf_train_test(X_train, X_test, y_train, y_test)]\n",
    "#\n",
    "print(f'80% train-test split accuracy is {np.mean(accuracies):.3f} {chr(177)}{np.std(accuracies):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
