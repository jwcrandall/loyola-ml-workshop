{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [20 pts] Define each of the following machine learning terms:\n",
    "\n",
    "* *dataset* - A collection of instances is a dataset and when working with machine learning methods we typically need a few datasets for different purposes.\n",
    "  * *instance* - a single row of data is called an instance. It is an observation from the domain.\n",
    "\n",
    "* *training* - a dataset that we feed into our machine learning algorithm to train our model\n",
    "\n",
    "* *testing* - a dataset that we use to validate the accuracy of our model but is not used to train the model. It may be called the validation dataset.\n",
    "* *validation dataset* - used to tune the models hyper-parameters.\n",
    "\n",
    "* *ground truth* - the value measured for your target variable for the training and testing examples where nearly all the time you can safely treat this the same as the label. If you augment your data set, there is a subtle difference between the ground truth (your actual measurements) and how the augmented examples relate to the labels you have assigned. \n",
    "\n",
    "* *label* - a human or machine generated description tagged to one or many data samples.\n",
    "\n",
    "* *pre-processing* - data transforming or encoding to bring data to a state such that a machine can easily parse it.\n",
    "\n",
    "* *feature* - an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.\n",
    "\n",
    "* *numerical* - numerical or quantitative data will always be a number that can be measured.\n",
    "\n",
    "* *nominal* - nominal data is classified without a natural rank, whereas ordinal data has a predetermined or natural order.\n",
    "\n",
    "* *decision surface* - A hyper surface in a multidimensional state space that partitions the space into  regions. Data lying on one side of a decision surface are defined as belonging to a different class form those lying on the the other. Decision surfaces may be created or modified as a result of a learning process and they are frequently used in machine learning, pattern recognition, and classification systems\n",
    "\n",
    "* *model validation* - the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.\n",
    "\n",
    "* *accuracy* - is a weighted arithmetic mean of precision and inverse precision (weighted by bias) as well as weighted arithmetic mean of recall and inverse recall (weighted by prevalence). Inverse precision and inverse recall are simply the precision and recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as ROC curves and provide a principled mechanism to explore operating point trade offs.\n",
    "\n",
    "* *cross-validation* - is a technique for evaluating ML models by training several ML models on a subset of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect over fitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [20 pts] Pick **two** of the [Scikit-learn datasets] which are already included in the library (i.e. the ones with datasets_load_) an find out the following:\n",
    "* the number of data points\n",
    "* the number of features and their types\n",
    "* the number and name of categories (i.e. the target field)\n",
    "* the mean (or mode if nominal) of the first two features\n",
    "\n",
    "[Scikit-learn datasets]: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mnumber of data points:\u001b[0m\n",
      "alcohol                         178\n",
      "malic_acid                      178\n",
      "ash                             178\n",
      "alcalinity_of_ash               178\n",
      "magnesium                       178\n",
      "total_phenols                   178\n",
      "flavanoids                      178\n",
      "nonflavanoid_phenols            178\n",
      "proanthocyanins                 178\n",
      "color_intensity                 178\n",
      "hue                             178\n",
      "od280/od315_of_diluted_wines    178\n",
      "proline                         178\n",
      "target                          178\n",
      "dtype: int64\n",
      "\u001b[1m\u001b[32mnumber of features:\u001b[0m\n",
      "14\n",
      "\u001b[1m\u001b[32mtype of feature:\u001b[0m\n",
      "alcohol                         float64\n",
      "malic_acid                      float64\n",
      "ash                             float64\n",
      "alcalinity_of_ash               float64\n",
      "magnesium                       float64\n",
      "total_phenols                   float64\n",
      "flavanoids                      float64\n",
      "nonflavanoid_phenols            float64\n",
      "proanthocyanins                 float64\n",
      "color_intensity                 float64\n",
      "hue                             float64\n",
      "od280/od315_of_diluted_wines    float64\n",
      "proline                         float64\n",
      "target                            int64\n",
      "dtype: object\n",
      "\u001b[1m\u001b[32mnumber of categories:\u001b[0m\n",
      "3\n",
      "\u001b[1m\u001b[32mname of categories: \u001b[0m\n",
      "['class_0' 'class_1' 'class_2']\n",
      "\u001b[1m\u001b[32mmean of alcohol: \u001b[0m\n",
      "<bound method Series.mean of 0      14.23\n",
      "1      13.20\n",
      "2      13.16\n",
      "3      14.37\n",
      "4      13.24\n",
      "       ...  \n",
      "173    13.71\n",
      "174    13.40\n",
      "175    13.27\n",
      "176    13.17\n",
      "177    14.13\n",
      "Name: alcohol, Length: 178, dtype: float64>\n",
      "\u001b[1m\u001b[32mmean of malic_acid: \u001b[0m\n",
      "<bound method Series.mean of 0      1.71\n",
      "1      1.78\n",
      "2      2.36\n",
      "3      1.95\n",
      "4      2.59\n",
      "       ... \n",
      "173    5.65\n",
      "174    3.91\n",
      "175    4.28\n",
      "176    2.59\n",
      "177    4.10\n",
      "Name: malic_acid, Length: 178, dtype: float64>\n"
     ]
    }
   ],
   "source": [
    "data = load_wine(as_frame=True)\n",
    "print(colored('number of data points:', 'green', attrs=['bold']))\n",
    "print(data.frame.count())\n",
    "print(colored('number of features:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes.size)\n",
    "print(colored('type of feature:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes)\n",
    "print(colored('number of categories:', 'green', attrs=['bold']))\n",
    "print(data.target_names.size)\n",
    "print(colored('name of categories: ', 'green', attrs=['bold']))\n",
    "print(data.target_names)\n",
    "print(colored('mean of alcohol: ', 'green', attrs=['bold']))\n",
    "print(data.frame['alcohol'].mean)\n",
    "print(colored('mean of malic_acid: ', 'green', attrs=['bold']))\n",
    "print(data.frame['malic_acid'].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mnumber of data points:\u001b[0m\n",
      "MedInc         20640\n",
      "HouseAge       20640\n",
      "AveRooms       20640\n",
      "AveBedrms      20640\n",
      "Population     20640\n",
      "AveOccup       20640\n",
      "Latitude       20640\n",
      "Longitude      20640\n",
      "MedHouseVal    20640\n",
      "dtype: int64\n",
      "\u001b[1m\u001b[32mnumber of features:\u001b[0m\n",
      "9\n",
      "\u001b[1m\u001b[32mtype of feature:\u001b[0m\n",
      "MedInc         float64\n",
      "HouseAge       float64\n",
      "AveRooms       float64\n",
      "AveBedrms      float64\n",
      "Population     float64\n",
      "AveOccup       float64\n",
      "Latitude       float64\n",
      "Longitude      float64\n",
      "MedHouseVal    float64\n",
      "dtype: object\n",
      "\u001b[1m\u001b[32mnumber of categories:\u001b[0m\n",
      "20640\n",
      "\u001b[1m\u001b[32mname of categories: \u001b[0m\n",
      "['MedHouseVal']\n",
      "\u001b[1m\u001b[32mmean of MedInc: \u001b[0m\n",
      "<bound method Series.mean of 0        8.3252\n",
      "1        8.3014\n",
      "2        7.2574\n",
      "3        5.6431\n",
      "4        3.8462\n",
      "          ...  \n",
      "20635    1.5603\n",
      "20636    2.5568\n",
      "20637    1.7000\n",
      "20638    1.8672\n",
      "20639    2.3886\n",
      "Name: MedInc, Length: 20640, dtype: float64>\n",
      "\u001b[1m\u001b[32mmean of HouseAge: \u001b[0m\n",
      "<bound method Series.mean of 0        41.0\n",
      "1        21.0\n",
      "2        52.0\n",
      "3        52.0\n",
      "4        52.0\n",
      "         ... \n",
      "20635    25.0\n",
      "20636    18.0\n",
      "20637    17.0\n",
      "20638    18.0\n",
      "20639    16.0\n",
      "Name: HouseAge, Length: 20640, dtype: float64>\n"
     ]
    }
   ],
   "source": [
    "data = sklearn.datasets.fetch_california_housing(as_frame=True)\n",
    "print(colored('number of data points:', 'green', attrs=['bold']))\n",
    "print(data.frame.count())\n",
    "print(colored('number of features:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes.size)\n",
    "print(colored('type of feature:', 'green', attrs=['bold']))\n",
    "print(data.frame.dtypes)\n",
    "print(colored('number of categories:', 'green', attrs=['bold']))\n",
    "print(data.target.size)\n",
    "print(colored('name of categories: ', 'green', attrs=['bold']))\n",
    "print(data.target_names)\n",
    "print(colored('mean of MedInc: ', 'green', attrs=['bold']))\n",
    "print(data.frame['MedInc'].mean)\n",
    "print(colored('mean of HouseAge: ', 'green', attrs=['bold']))\n",
    "print(data.frame['HouseAge'].mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [40 pts] Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file (not provided, you have to download it by yourself by following the instructions in the module Jupyter notebook). Display the correlation matrix where each row and column are the features, which should be an 8 by 8 matrix (should we use 'Serial no'?). You can use pandas DataFrame.corr() to verify correctness of yours.\n",
    "\n",
    "Observe that the diagonal of this matrix should have all 1's and explain why? Since the last column can be used as the target (dependent) varaible, what do you think about the correlations between all the variables? Which variable should be the most important for prediction of 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. [20 pts] Classification of mushrooms, edible or poisonous. Download the *assignment01_mushroom_dataset.csv* dataset file from the module content. Load the data set in your model development framework, examine the features to se they are all nominal features. The first column is the calss which represents the mishroom is poisonous or not. Apply necessary pre-processing such as nominal to numerical conversions. make sure sanity check the pipeline the perhaps run your favorite baseline classifer first.\n",
    "\n",
    "Report the performance of your classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
