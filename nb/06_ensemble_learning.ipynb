{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T13:59:28.560174Z",
     "start_time": "2020-08-13T13:59:28.556379Z"
    }
   },
   "source": [
    "# Ensemble Learning\n",
    "## Introduction\n",
    "Ensemble methods combine multiple hypotheses, aiming to form a better one than the best hypothesis alone. Often, ensemble methods use multiple weak learners to build a strong learner.\n",
    "\n",
    "A **weak learning** is one that consistently generates better predictions than random.\n",
    "\n",
    "Ensemble methods use multiple learning algorithms (or instances) to obtain better predictive performance than could be obtained from any of the individual learning algorithms (or instances) alone. A machine learning ensemble can consist of a finite set of alternative models, which are more general classification models discretely.\n",
    "\n",
    "The common ensemble training scheme employs numerous weaker classifiers that use only subsets of the dataset in terms of rows (data points) or columns (features), and then conducts a majority voting among their resulting predictions to decide on a particular single data point classification.\n",
    "\n",
    "One important advantage of ensemble methods is the reduction of over-fitting.\n",
    "\n",
    "## Bagging or Bootstrap Aggregating\n",
    "Bagging trains each model in the ensemble using a randomly drawn subset of the training set. Then lets each model in the ensemble vote with equal weight. The random forest algorithm combines random decision trees with bagging to achieve high classification accuracy while preserving model generality, which is one of the reasons why the Random Forest classifier is very popular.\n",
    "\n",
    "## Curse of Dimensionality\n",
    "In machine learning one of the biggest difficulty is dealing with high number of dataset features or lack of enough data points to cover them. As an example, when a naive model requires $5$ data points on each feature of a $100$ feature dataset, the **joint** density function would ideally necessitate $5^{100}$ data points, so that the density function could be computed perfectly. This is not only too big of a dataset size but also it might be impossible to acquire such a dataset. In practice, the curse of dimensionality is avoided by assuming the features are independent, such as the Naive Bayes assumption. The alternative approach is using ensembles of weak classifiers and bagging, so that each classifier would work on a subset of features.\n",
    "\n",
    "## Boosting\n",
    "Boosting trains several weak learning algorithms and combine (i.e. summation) their weighted results. Boosting builds an ensemble by training each new model instance in such a way to improve upon the previous models mis-classify. The most common implementation of boosting is Adaboost where the classifier is composed of $T$ many classifiers such that $F_T(x) = \\sum_{t=1}^{T} f_t(x)$. The training is done such that the error is minimized at every iteration, $E_T = \\sum_i E (F_{t-1}(x_i) + \\alpha_t h(x_i)$\n",
    "\n",
    "## Random Forest\n",
    "he Random Forest classifier is a method that combines the decision trees and ensemble learning. The forest is composed of many trees that use randomly picked data features (attributes) as their input. The forest generation process constructs a collection of trees with controlled variance. The resulting prediction can be decided by majority voting or weighted voting. Random Forests have several advantages, such as a low number of control and model parameters, resistance to overfitting, no requirement for feature selection or feature reduction because they can use a large number of potential attributes. If some features are not useful for prediction, the algorithm will simply ignore them. One important advantage of Random Forest is that the variance of the model decreases as the number of trees in the forest increase, whereas the bias remains the same.\n",
    "\n",
    "Random Forests have some disadvantages such as low model interpretability, performance loss due to correlated (dependent) variables, and dependence on the random number generator of the implementation. Note that all these disadvantages (except for the third one which is relatively easy to fix) are inherent to several other popular classifiers.\n",
    "\n",
    "### Implementation Details\n",
    "Number of features in each tree can be set to either $\\sqrt{M}$, or $\\log_2(M+1)$ for a dataset $X \\in \\mathbb{R}^{N \\times M}$\n",
    "\n",
    "Ensemble size determination remains to be a challenge. In practice a size is picked subjectively according to the number of features and data points.\n",
    "\n",
    "## Decision Tree Learning\n",
    "Given: Collection of examples ($x, f(x)$)\n",
    "Return: a function *h* (hypothesis) that approximates $f$, where $h$ is a decision tree\n",
    "\n",
    "Note that $f(x)$ is generally know and a dataset (of observations or records) is considered. In our common problems, we have $X$ as the dataset and $y$ as the observations (ground truth labels).\n",
    "\n",
    "Input: An object or situation described by a set of attributes (or features), i.e. the dataset $X$\n",
    "Output: a \"decision\" $\\rightarrow$ predicts the output value for the input, i.e. $y$\n",
    "\n",
    "For a decision tree learner, the input attributes can be nominal (discrete) or numerical (continuous).\n",
    "\n",
    "A decision tree is a tree with two types of nodes  \n",
    "* Decision nodes, specifies a test on a feature (attribute) with 2 or more alternatives\n",
    "  * Every decision node is in the path to a leaf node  \n",
    "* Leaf nodes, indicates a classification of a data point (example, test)\n",
    "\n",
    "The following cells demonstrate how to use this data in a regular classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:34:44.086843Z",
     "start_time": "2020-08-13T17:34:43.542935Z"
    },
    "execution": {
     "iopub.status.idle": "2020-08-17T20:32:05.116433Z",
     "shell.execute_reply": "2020-08-17T20:32:05.115755Z",
     "shell.execute_reply.started": "2020-08-17T20:32:04.375054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Speedy</th>\n",
       "      <th>Price</th>\n",
       "      <th>Will_Tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Food  Speedy  Price Will_Tip\n",
       "0     2       1      1      yes\n",
       "1     2       0      0       no\n",
       "2     2       1      0      yes\n",
       "3     2       0      1      yes\n",
       "4     1       1      1       no"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder  # Generally used for supervised learning targets\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.array([['good', 'yes', 'ok'],\n",
    "              ['good', 'no', 'high'],\n",
    "              ['good', 'yes', 'high'],\n",
    "              ['good', 'no', 'ok'],\n",
    "              ['ok', 'yes', 'ok'],\n",
    "              ['bad', 'yes', 'ok'],\n",
    "              ['good', 'yes', 'ok'],\n",
    "              ['good', 'yes', 'high'],\n",
    "              ['ok', 'yes', 'ok'],\n",
    "              ['good', 'yes', 'ok']\n",
    "             ]),\n",
    "    columns=['Food', 'Speedy', 'Price'])\n",
    "\n",
    "df['Will_Tip'] = pd.Series(np.array(['yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes']))\n",
    "\n",
    "# Convert nominals to numerical keeping the order - sklearn requirement for DecisionTreeClassifier\n",
    "df['Food'] = df['Food'].replace({'bad':0, 'ok':1, 'good':2})\n",
    "df['Speedy'] = df['Speedy'].replace({'no':0, 'yes':1})\n",
    "df['Price'] = df['Price'].replace({'high':0, 'ok':1})\n",
    "\n",
    "# Prepare X and y\n",
    "X = df.loc[:, df.columns != 'Will_Tip'].values\n",
    "y = df.loc[:, df.columns == 'Will_Tip'].values.ravel()\n",
    "\n",
    "# Convert y values to 0, 1 for sklearn\n",
    "y_le = LabelEncoder()\n",
    "y = y_le.fit_transform(y)\n",
    "\n",
    "# Sanity\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:34:44.145098Z",
     "start_time": "2020-08-13T17:34:44.088673Z"
    },
    "execution": {
     "iopub.execute_input": "2020-08-17T20:32:05.119040Z",
     "iopub.status.busy": "2020-08-17T20:32:05.118761Z",
     "iopub.status.idle": "2020-08-17T20:32:05.204904Z",
     "shell.execute_reply": "2020-08-17T20:32:05.204062Z",
     "shell.execute_reply.started": "2020-08-17T20:32:05.119020Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "willtip_clf = DecisionTreeClassifier(random_state=0)\n",
    "model = willtip_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualization requires `graphviz` which can be installed by `conda install python-graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T18:10:48.080878Z",
     "start_time": "2020-08-13T18:10:48.055333Z"
    },
    "execution": {
     "iopub.execute_input": "2020-08-17T20:32:05.207149Z",
     "iopub.status.busy": "2020-08-17T20:32:05.206984Z",
     "iopub.status.idle": "2020-08-17T20:32:05.661526Z",
     "shell.execute_reply": "2020-08-17T20:32:05.657785Z",
     "shell.execute_reply.started": "2020-08-17T20:32:05.207131Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wt_tree.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ce142fe1617f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Display the decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'wt_tree.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0;32m-> 1225\u001b[0;31m                 metadata=metadata)\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wt_tree.png'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "from subprocess import call\n",
    "\n",
    "# export_graphviz will generate a DOT image file in the folder with this notebook\n",
    "dot_data = export_graphviz(willtip_clf,\n",
    "                           out_file='wt_tree.dot',\n",
    "                           feature_names=df.columns[:-1],\n",
    "                           class_names=y_le.classes_,\n",
    "                           filled=True,\n",
    "                           rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Convert the DOT file to PNG\n",
    "call(['dot', '-Tpng', 'wt_tree.dot', '-o', 'wt_tree.png', '-Gdpi=60'], shell=True)\n",
    "\n",
    "# Display the decision tree\n",
    "Image(filename = 'wt_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Example: Following is another decision example using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:02.693640Z",
     "start_time": "2020-08-13T17:35:02.620868Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.662661Z",
     "iopub.status.idle": "2020-08-17T20:32:05.663243Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# \n",
    "iris_clf = DecisionTreeClassifier(random_state=0)\n",
    "model = iris_clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Generate the learned tree using DOT language and graphviz\n",
    "dot_data = export_graphviz(iris_clf,\n",
    "                           out_file='iris_tree.dot',\n",
    "                           feature_names=iris.feature_names,\n",
    "                           class_names=iris.target_names,\n",
    "                           filled=True,\n",
    "                           rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Convert the DOT file to PNG\n",
    "call(['dot', '-Tpng', 'iris_tree.dot', '-o', 'iris_tree.png', '-Gdpi=60'], shell=True)\n",
    "\n",
    "# Display the decision tree\n",
    "Image(filename = 'iris_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ID3 Algorithm: Building a Decision Tree\n",
    "**Hard problem:** If the dataset has $M$ *binary* features then there are $2^M$ rows in the *complete* truth table. In addition, if the outcome, i.e. target variable, is binary then each row can have a ${0,1}$ output. The numbers of possible ways to build this truth table is $2^2^M$. Thus the solution space is exponential, and the decision tree algorithms are always have to be *greedy* as there is no known method to solve them in polynomial time.\n",
    "\n",
    "All greedy algorithms have a **strategy**.\n",
    "\n",
    "**Occam's Razor**, the principle of parsimony, is the principle that one should not make more assumptions than the minimum needed.\n",
    "\n",
    "All scientific modeling and theory building is based on this principle.\n",
    "\n",
    "Another interpretation, all other things being equal, pick the smallest explanation.\n",
    "\n",
    "Building a decision tree using Occam's Razor principle results in the smallest tree possible. The approach also minimizes overfitting.\n",
    "\n",
    "Algorithm idea: Recursively pick the *most significant* attribute as root of the (sub)tree\n",
    "* Use a top-down greedy search through the space of possible decision trees\n",
    "* Greedy strategy - pick the highest/best value first\n",
    "\n",
    "Measure of the worthiness of an attribute: Information Gain\n",
    "\n",
    "$I=-\\sum_{i=1}^{K} P\\left(y_{i}\\right) \\log _{2}\\left(P\\left(y_{i}\\right)\\right)$\n",
    "\n",
    "$I\\left(\\frac{p}{p+n}, \\frac{n}{p+n}\\right)=-\\frac{p}{p+n} \\log _{2}\\left(\\frac{p}{p+n}\\right)-\\frac{n}{p+n} \\log _{2}\\left(\\frac{n}{p+n}\\right)$\n",
    "\n",
    "$I$ is also called **entropy** of the prior.\n",
    "\n",
    "A good strategy is starting with the feature that has the highest entropy of the dependent variable.\n",
    "* *Information* answers questions. The more clueless I am about a question, the more information the answer contains.\n",
    "\n",
    "**Intuition**: Pick the attribute that reduces the entropy (uncertainty) the most.\n",
    "\n",
    "$\\mathrm{IG}(\\mathrm{A})=\\mathrm{I}-\\mathrm{remainder}(\\mathrm{A})$\n",
    "\n",
    "$\\mathrm{A}$ is the remaining uncertainty after getting info (i.e. using on a decision tree to decide, split)\n",
    "\n",
    "$\\text { remainder }(\\mathrm{A})=\\sum_{i=1}^{K} \\frac{p_{i}+n_{i}}{p+n} I\\left(\\frac{p_{i}}{p+n}, \\frac{n_{i}}{p+n}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "## Demonstrating the Power of Numerous Weak Classifiers as a Group\n",
    "Note that not all datasets would benefit an ensemble classifier. The following cells demonstrate the power of the ensemble classifier with the following steps.\n",
    "* Data exploration\n",
    "* Evaluating regular classifiers on the dataset\n",
    "* Building an ensemble\n",
    "\n",
    "Recall that a few strategies to build a weak learner:\n",
    "\n",
    "* Use a subset of dataset in training\n",
    "* Use a subset of features in training\n",
    "* Use a weak learner (generally more primitive and have higher generalization, or abstraction)\n",
    "\n",
    "Note that the test on the weak learner has to match the subset features that the learning model uses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:05.616908Z",
     "start_time": "2020-08-13T17:35:05.588008Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.664593Z",
     "iopub.status.idle": "2020-08-17T20:32:05.665020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Locate and load the data file\n",
    "df = pd.read_csv('../datasets/module06_titanic_preprocessed.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(f'#rows={len(df)}, #columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:07.556814Z",
     "start_time": "2020-08-13T17:35:06.908664Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.666244Z",
     "iopub.status.idle": "2020-08-17T20:32:05.666458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data exploration, a few plots\n",
    "def plt_var(_col):\n",
    "    plt.hist([df[df['Survived']==1][_col],df[df['Survived']==0][_col]], label=['Suvd','Not Suvd'])\n",
    "    plt.xlabel(_col)\n",
    "    plt.legend()    \n",
    "#\n",
    "plt.figure(figsize=(18, 3))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt_var('Age')\n",
    "#\n",
    "plt.subplot(1, 5, 2)\n",
    "plt_var('Sex')\n",
    "#\n",
    "plt.subplot(1, 5, 3)\n",
    "plt_var('Fare')\n",
    "#\n",
    "plt.subplot(1, 5, 4)\n",
    "plt_var('Pclass_1')\n",
    "#\n",
    "plt.subplot(1, 5, 5)\n",
    "plt_var('Pclass_3')\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Evaluate numerous classifiers** by a stratified 10-fold CV as in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:10.729696Z",
     "start_time": "2020-08-13T17:35:10.724152Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.667727Z",
     "iopub.status.idle": "2020-08-17T20:32:05.668109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the input X and y\n",
    "dfX = df.loc[:, df.columns != 'Survived']\n",
    "dfy = df.loc[:, df.columns == 'Survived'].values.ravel()\n",
    "\n",
    "# Sanity check\n",
    "print(f'N={len(dfX)}, M={len(dfX.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:11.239598Z",
     "start_time": "2020-08-13T17:35:11.237164Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.669019Z",
     "iopub.status.idle": "2020-08-17T20:32:05.669341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set our main data structures X and y\n",
    "X = dfX.values\n",
    "y = dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:11.929072Z",
     "start_time": "2020-08-13T17:35:11.923903Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.670193Z",
     "iopub.status.idle": "2020-08-17T20:32:05.670396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 10-fold CV evaluation of a classifier\n",
    "def eval_classifier(_clf, _X, _y):\n",
    "    acc = []\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "    for train_index, test_index in kf.split(_X, _y):\n",
    "        _clf.fit(_X[train_index], _y[train_index])\n",
    "        y_pred = _clf.predict(_X[test_index])\n",
    "        acc += [accuracy_score(_y[test_index], y_pred)]\n",
    "    return np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:24.123937Z",
     "start_time": "2020-08-13T17:35:24.101042Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.671433Z",
     "iopub.status.idle": "2020-08-17T20:32:05.671646Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "acc = eval_classifier(GaussianNB(),\n",
    "                      X, y)\n",
    "print(f'Naive Bayes CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:31.046292Z",
     "start_time": "2020-08-13T17:35:30.860435Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.672480Z",
     "iopub.status.idle": "2020-08-17T20:32:05.672766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Problem is difficult and LinearSVC does not converge\n",
    "# Increase the convergence tolerance to a very high number\n",
    "acc = eval_classifier(LinearSVC(tol=10),\n",
    "                      X, y)\n",
    "print(f'Linear SVC CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:41.143956Z",
     "start_time": "2020-08-13T17:35:41.021850Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.673679Z",
     "iopub.status.idle": "2020-08-17T20:32:05.673948Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# See balancing the classes with penalties help or not\n",
    "acc = eval_classifier(LinearSVC(class_weight='balanced', tol=10),\n",
    "                      X, y)\n",
    "print(f'Linear SVC CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:46.585763Z",
     "start_time": "2020-08-13T17:35:46.022458Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.674915Z",
     "iopub.status.idle": "2020-08-17T20:32:05.675120Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "acc = eval_classifier(SVC(kernel='rbf', gamma=.5, C=2),\n",
    "                      X, y)\n",
    "print(f'SVM RBF CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:35:55.907842Z",
     "start_time": "2020-08-13T17:35:55.461949Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.676133Z",
     "iopub.status.idle": "2020-08-17T20:32:05.676405Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "acc = eval_classifier(SVC(kernel='rbf', gamma=.05, C=2),\n",
    "                      X, y)\n",
    "print(f'SVM RBF CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:03.369262Z",
     "start_time": "2020-08-13T17:36:01.614514Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.677210Z",
     "iopub.status.idle": "2020-08-17T20:32:05.677451Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "acc = eval_classifier(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter =1000, class_weight='balanced'),\n",
    "                      X, y)\n",
    "print(f'Logistic Regression CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:11.353654Z",
     "start_time": "2020-08-13T17:36:07.885864Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.678396Z",
     "iopub.status.idle": "2020-08-17T20:32:05.678658Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "acc = eval_classifier(MLPClassifier(hidden_layer_sizes=(50,100),max_iter=500),\n",
    "                      X, y)\n",
    "print(f'Neural Network CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:21.007563Z",
     "start_time": "2020-08-13T17:36:16.527485Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.679635Z",
     "iopub.status.idle": "2020-08-17T20:32:05.679839Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = eval_classifier(MLPClassifier(hidden_layer_sizes=(20,50,50,20),max_iter=2000),\n",
    "                      X, y)\n",
    "print(f'Neural Network CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:25.795009Z",
     "start_time": "2020-08-13T17:36:25.734369Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.680741Z",
     "iopub.status.idle": "2020-08-17T20:32:05.680974Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "acc = eval_classifier(KNeighborsClassifier(n_neighbors=1),\n",
    "                      X, y)\n",
    "print(f'K Nearest Neighbor CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:30.157502Z",
     "start_time": "2020-08-13T17:36:30.120125Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.682153Z",
     "iopub.status.idle": "2020-08-17T20:32:05.682473Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = eval_classifier(DecisionTreeClassifier(),\n",
    "                      X, y)\n",
    "print(f'Decision Tree CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:36:39.183019Z",
     "start_time": "2020-08-13T17:36:35.295166Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.683380Z",
     "iopub.status.idle": "2020-08-17T20:32:05.683611Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "acc = eval_classifier(RandomForestClassifier(n_estimators=200, max_depth=5, random_state=None, n_jobs=4),\n",
    "                      X, y)\n",
    "print(f'Random Forest CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:37:06.604943Z",
     "start_time": "2020-08-13T17:37:06.601724Z"
    }
   },
   "source": [
    "Best result so far..\n",
    "\n",
    "---\n",
    "\n",
    "**Build an ensemble** using regular Naive Bayes (NB) classifier as the weak learner. As can be seen above, its performance is almost half of the Random Forest ensemble classifier.\n",
    "\n",
    "The following approach uses a subset of features to generate a Naive Bayes classifier. Note that `_clf` is the weak classifier model and its `predict` function will be used. It is only called a weak model because a subset of features are passed for learning.\n",
    "\n",
    "We also compute the original class `Priors` from the full dataset to be fair to the NB learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:41:12.642359Z",
     "start_time": "2020-08-13T17:41:12.639036Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.684678Z",
     "iopub.status.idle": "2020-08-17T20:32:05.684897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main Priors\n",
    "counts = np.unique(y, return_counts=True)\n",
    "NBpriors = [counts[1][0]/len(y), counts[1][1]/len(y)]\n",
    "#\n",
    "print(NBpriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:41:21.809981Z",
     "start_time": "2020-08-13T17:41:17.912453Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.685831Z",
     "iopub.status.idle": "2020-08-17T20:32:05.686124Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = eval_classifier(GaussianNB(priors=NBpriors), X, y)\n",
    "print(f'Naive Bayes CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')\n",
    "\n",
    "# For reference\n",
    "acc = eval_classifier(RandomForestClassifier(n_estimators=200, max_depth=5, random_state=None, n_jobs=4), X, y)\n",
    "print(f'Random Forest CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:41:37.005526Z",
     "start_time": "2020-08-13T17:41:37.000498Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.686959Z",
     "iopub.status.idle": "2020-08-17T20:32:05.687196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's build a weak learner using GaussianNB by subset of features\n",
    "def weakNB_fit(_list_cols, _X, _y):\n",
    "    Xs = _X[:,_list_cols]\n",
    "    return GaussianNB(priors=NBpriors).fit(Xs, _y)\n",
    "\n",
    "def weakNB_predict(_clf, _list_cols, _X):\n",
    "    Xs = _X[:,_list_cols]\n",
    "    return _clf.predict(Xs), _clf.predict_proba(Xs)\n",
    "\n",
    "# Use _m features randomly selected, a total of M weak learners\n",
    "def features_randomsubset(_M, _m, nensemble=1):\n",
    "    from numpy.random import choice\n",
    "    # returns a list of list of column choices - subset features\n",
    "    return [choice(_M, _m, replace=False) for _ in range(nensemble)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:41:44.146478Z",
     "start_time": "2020-08-13T17:41:42.985409Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.688036Z",
     "iopub.status.idle": "2020-08-17T20:32:05.688254Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def eval_weak(_X, _y, _nensemble, _nfeatures):\n",
    "    acc = []\n",
    "    for j in range(_nensemble):\n",
    "        # Keep subset features, columns same for a 10-fold\n",
    "        cols = features_randomsubset(_X.shape[1], _nfeatures, nensemble=1)\n",
    "        # 10-fold CV\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "        for train_index, test_index in kf.split(_X, _y):\n",
    "            clf = weakNB_fit(cols[0], _X[train_index], _y[train_index])\n",
    "            y_pred, y_prob = weakNB_predict(clf, cols[0], _X[test_index])\n",
    "            acc += [accuracy_score(_y[test_index], y_pred)]\n",
    "    #\n",
    "    return np.array(acc)\n",
    "    \n",
    "# Measure individual weak learners performance\n",
    "acc = eval_weak(X, y, 100, 5)\n",
    "\n",
    "# Sanity\n",
    "print(f'total #results={len(acc)}')\n",
    "#\n",
    "print(f'Weak learners average Acc= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, by using the good old `GaussianNB` with a small dataset and accumulating its accuracy shows a jump in performance compared to the plain NB using all features at the same time.\n",
    "\n",
    "**Question 1:** Why the performance increase?  \n",
    "**Question 2:** Why the increase in error standard deviation?\n",
    "\n",
    "Note: Attempting number of features `nfeatures=3` or so might cause individual `GaussianNB` classifier to fail due to data problems with that particular subset of features (such as no data point for the target, or all same values for a column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:42:48.490651Z",
     "start_time": "2020-08-13T17:42:48.483110Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.689059Z",
     "iopub.status.idle": "2020-08-17T20:32:05.689338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate numerous trained NB classifiers based on subset features\n",
    "def ensembleNB_fit(_ensemble_cols, _X, _y):\n",
    "    # the list of ensemble columns have a column list for every member of the ensemble\n",
    "    nensemble = len(_ensemble_cols)\n",
    "    # list of weak learners\n",
    "    ensemble_clf = []\n",
    "    for j in range(nensemble):\n",
    "        ensemble_clf += [weakNB_fit(_ensemble_cols[j], _X, _y)]\n",
    "    #\n",
    "    return ensemble_clf\n",
    "\n",
    "# Using learned ensemble predict the outcome with majority voting\n",
    "def ensembleNB_predict(_ensemble_clf, _ensemble_cols, _Xtest):\n",
    "    from collections import defaultdict\n",
    "    nensemble = len(_ensemble_clf)\n",
    "    assert nensemble==len(_ensemble_cols)  # Error check\n",
    "    # weak learner predictions\n",
    "    ypred_e, yprob_e = [], []\n",
    "    for j in range(nensemble):\n",
    "        res = weakNB_predict(_ensemble_clf[j], _ensemble_cols[j], _Xtest)\n",
    "        ypred_e += [res[0]]\n",
    "        yprob_e += [res[1]]  # score of the prediction\n",
    "    # majority voting for each data point in _Xtest\n",
    "    ypred = []\n",
    "    for i in range(_Xtest.shape[0]):\n",
    "        ypred_scores = defaultdict(float)\n",
    "        for j in range(nensemble):\n",
    "            for c, p in enumerate(yprob_e[j][i]):\n",
    "                # a proper score is necessary\n",
    "                ypred_scores[c] += p\n",
    "        ix = max(ypred_scores.items(), key=lambda a: a[1])\n",
    "        ypred += [ix[0]]\n",
    "    #\n",
    "    return np.array(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:43:18.031044Z",
     "start_time": "2020-08-13T17:42:57.691294Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.690166Z",
     "iopub.status.idle": "2020-08-17T20:32:05.690387Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def eval_ensemble(_X, _y, _niter, _nensemble, _nfeatures):\n",
    "    acc = []\n",
    "    for i in range(_niter):\n",
    "        # Keep subset features, columns same for a 10-fold\n",
    "        cols = features_randomsubset(_X.shape[1], _nfeatures, nensemble=_nensemble)\n",
    "        # 10-fold CV\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "        for train_index, test_index in kf.split(_X, _y):\n",
    "            clf = ensembleNB_fit(cols, _X[train_index], _y[train_index])\n",
    "            y_pred = ensembleNB_predict(clf, cols, _X[test_index])\n",
    "            acc += [accuracy_score(_y[test_index], y_pred)]\n",
    "    #\n",
    "    return np.array(acc)\n",
    "#\n",
    "# Measure ensemble weak learners performance\n",
    "acc = eval_ensemble(X, y, 10, 200, 7)\n",
    "#\n",
    "print(f'Ensemble learners average Acc= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Above results are not very promising, so do you think we have a bug in the scripts?\n",
    "\n",
    "---\n",
    "\n",
    "## Improvements\n",
    "More or less half of the features are not useful in this dataset. So, subsets of them are not useful either. Random Forest classifier ignores many of these by employing **entropy** or **information gain** strategy. However such capability does not exist for a plain NB classifier. Thus, we need to do some kind of feature ranking.\n",
    "\n",
    "Use a simple approach and check the **correlation** between `X` columns and the target variable `y`. Drop the bottom performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:45:07.399552Z",
     "start_time": "2020-08-13T17:45:07.392746Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.691298Z",
     "iopub.status.idle": "2020-08-17T20:32:05.691520Z"
    }
   },
   "outputs": [],
   "source": [
    "# Direct correlation between each column of X and the target y\n",
    "corrs = np.array([np.correlate(X[:,j], y)[0] for j in range(X.shape[1])])\n",
    "\n",
    "# Reverse sort, numpy array negation reverses the order\n",
    "ranks = np.argsort((-corrs))\n",
    "\n",
    "# Display top-9 and bot-5\n",
    "rankings = [(f'{corrs[j]:.1f}', df.columns[j]) for j in ranks]\n",
    "display(rankings[:9])\n",
    "display(rankings[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:45:12.523477Z",
     "start_time": "2020-08-13T17:45:12.519697Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.692505Z",
     "iopub.status.idle": "2020-08-17T20:32:05.692943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find columns with correlation=2 or less and delete them from X\n",
    "delcols = [(j, f'{corrs[j]:.1f}', df.columns[j]) for j in ranks if corrs[j]<=2]\n",
    "#\n",
    "print(delcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:45:16.963642Z",
     "start_time": "2020-08-13T17:45:16.959782Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.693952Z",
     "iopub.status.idle": "2020-08-17T20:32:05.694286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column numbers to delete\n",
    "dd = [d[0] for d in delcols]\n",
    "\n",
    "# Drop those columns, axis=1\n",
    "Xpp = np.delete(np.array(X, copy=True), dd, axis=1)\n",
    "\n",
    "# Xpp is new pre-processed X\n",
    "\n",
    "# Sanity check\n",
    "print(f'{X.shape}, {Xpp.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:45:25.710412Z",
     "start_time": "2020-08-13T17:45:21.808045Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.695193Z",
     "iopub.status.idle": "2020-08-17T20:32:05.695476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base classifier\n",
    "acc = eval_classifier(GaussianNB(priors=NBpriors), Xpp, y)\n",
    "accNB, stdevNB = np.mean(acc), np.std(acc)\n",
    "print(f'Naive Bayes CV accuracy={accNB:.2f} {chr(177)}{stdevNB:.3f}')\n",
    "\n",
    "# Reference\n",
    "acc = eval_classifier(RandomForestClassifier(n_estimators=200, max_depth=5, random_state=None, n_jobs=4), Xpp, y)\n",
    "print(f'Random Forest CV accuracy={np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The plain Naive Bayes has an improved performance now, since we are helping NB with the removal of uncorrelated features. Random Forest uses information gain to do this by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:45:41.612394Z",
     "start_time": "2020-08-13T17:45:40.480095Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.696407Z",
     "iopub.status.idle": "2020-08-17T20:32:05.696929Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Measure weak learners performance on updated X\n",
    "acc = eval_weak(Xpp, y, 100, 5)\n",
    "print(f'Weak learners average Acc= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:46:07.599761Z",
     "start_time": "2020-08-13T17:45:46.790946Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.697887Z",
     "iopub.status.idle": "2020-08-17T20:32:05.698165Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = eval_ensemble(Xpp, y, 10, 200, 11)\n",
    "print(f'Ensemble learners average Acc= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:46:07.604116Z",
     "start_time": "2020-08-13T17:46:07.601992Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.699110Z",
     "iopub.status.idle": "2020-08-17T20:32:05.699335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "M = Xpp.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The following experiment takes around 12 minutes on my machine to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:58:51.049858Z",
     "start_time": "2020-08-13T17:46:28.601045Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.700475Z",
     "iopub.status.idle": "2020-08-17T20:32:05.700708Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run an experiment for a full scale of number of features\n",
    "valsF, accF, stdevF = np.arange(1,M+1), [], []\n",
    "for nf in valsF:\n",
    "    acc = eval_ensemble(Xpp, y, 10, 200, nf)\n",
    "    accF += [np.mean(acc)]\n",
    "    stdevF += [np.std(acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "Since we are varying the feature subset size for the ensemble NB, we can do something similar for the plain NB. Let's vary the features added for classification using the ranked features and plot the 10-fold CV performance. By ranking the features we are helping the plain Naive Bayes immensely.\n",
    "\n",
    "Note that both the ensemble test and plain NB test have 10 iterations to collect sufficient statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:58:54.468742Z",
     "start_time": "2020-08-13T17:58:51.052602Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.701545Z",
     "iopub.status.idle": "2020-08-17T20:32:05.701784Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add number of iterations - better statistics\n",
    "def eval_classifier_niter(_clf, _X, _y, _niter):\n",
    "    return np.array([eval_classifier(_clf, _X, _y) for i in range(_niter)])\n",
    "\n",
    "# Get the X matrix starting from the high-ranked variables\n",
    "def getX(_X, _ncols, _ranks):\n",
    "    cols = [j for n, j in enumerate(_ranks) if n < _ncols]\n",
    "    return np.array(_X[:,cols])\n",
    "\n",
    "# Iterate over the features, adding 1 by 1 by starting from the first (the best) feature\n",
    "valsNB, accNB, stdevNB = np.arange(1,M+1), [], []\n",
    "for nc in valsNB:\n",
    "    # feature order is coming from ranks\n",
    "    X_nb = getX(X, nc, ranks)\n",
    "    acc = eval_classifier_niter(GaussianNB(priors=NBpriors), X_nb, y, 10)\n",
    "    accNB += [np.mean(acc)]\n",
    "    stdevNB += [np.std(acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T17:58:54.614061Z",
     "start_time": "2020-08-13T17:58:54.472112Z"
    },
    "execution": {
     "iopub.status.busy": "2020-08-17T20:32:05.702817Z",
     "iopub.status.idle": "2020-08-17T20:32:05.703161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ease of computation for the plotting of the error band\n",
    "accF, stdevF = np.array(accF), np.array(stdevF)\n",
    "accNB, stdevNB = np.array(accNB), np.array(stdevNB)\n",
    "\n",
    "# Plot\n",
    "plt.plot(valsF, accF, label='Ensemble NB')\n",
    "plt.plot(valsNB, accNB, label='Plain NB')\n",
    "plt.fill_between(valsF, accF-stdevF, accF+stdevF, color='lavender', alpha=0.5)\n",
    "plt.fill_between(valsNB, accNB-stdevNB, accNB+stdevNB, color='papayawhip', alpha=0.5)\n",
    "plt.xlabel('Feature Subset Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "1. Ensemble performance is better than the plain NB when all features are added\n",
    "2. Ensemble has better generalization performance (why?)\n",
    "3. Ensemble performance has smaller error variance\n",
    "4. Ensemble uses all features randomly picked\n",
    "5. Plain NB starts adding the best features first, thus even with 1 feature its performance is 67%\n",
    "6. There is an optimum number of features for the NB\n",
    "7. The plain NB performance sensitivity to the features included is high (why?)\n",
    "8. The ensemble performance sensitivity to the features included is lower (why?)\n",
    "\n",
    "**Question:** From the above plot, the performance of the ensemble looks a bit less than the plain NB, do you agree with this statement? Why? \n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "**Exercise 6.1** Add the data points `['good', 'yes', 'ok', 'no']` to the data and rebuild the tree. Observe the difference between the previous and the current."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
