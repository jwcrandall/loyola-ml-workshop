{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __tensor__ is a mathematical object that has a generalized matrix to store data and interacts with other tensors in the same structure through transformations. A tensor in TensorFlow framework is a Lego piece and we use these pieces to build neural networks.\n",
    "\n",
    "In TensorFlow 1.0 the operations on the data were represented on _static_ __graph__. A similar framework PyTorch 1.0 has a __dynamic__ computation graph which can be updated in the run-time, thus provides more flexibility. Recently, TensorFlow 2.0 implemented a more user-friendly framework. After successful implementation of PyTorch, TensorFlow 2.0 followed the same suit for computational graphs and this module will cover TensorFlow 2.0.\n",
    "\n",
    "---\n",
    "\n",
    "Use a separate Anaconda environment for TensorFlow:  \n",
    "`conda create -n tf tensorflow jupyter matplotlib pandas scikit-learn`  \n",
    "`conda activate tf`  \n",
    "\n",
    "Run the notebook under the virtual environment `tf`.\n",
    "\n",
    "---\n",
    "\n",
    "TensorFlow API: https://www.tensorflow.org/api_docs/python/tf\n",
    "\n",
    "## TensorFlow Architecture\n",
    "In general, popular and fast Python libraries are coded in C++ programming language for speed. The TensorFlow GPU complementary library uses Nvidia CUDA and it requires CUDA SDK and `cuDNN` libraries to be installed on a computer with CUDA capable GPU hardware available.\n",
    "\n",
    "The Python low-level API wraps the C++ sources and makes it possible to perform basic operations such as matrix multiplication and convolutional filters.\n",
    "\n",
    "Top-level level API is made of two components, Keras and the Estimator API. Keras is a user-friendly, modular, and extensible wrapper for TensorFlow. The Estimator API contains several components that allow building ML models easily. As in any other ML methodology, in deep learning, a model usually refers to a neural network that was trained on data. Thus, a model is composed of a neural network architecture, matrix weights, and hyperparameters of the model.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "We have seen the multilayer perceptrons generally composed of `fully-connected` networks, that is, each neuron in one layer is connected to all neurons in the next layer, such as in the perceptron $y=w^{\\top} x+b$. In one perspective, the _fully-connectedness_ of these networks makes them prone to overfitting data. A novel invention came out as the Convolutional Neural Networks (CNN), which are considered regularized versions of multilayer perceptrons. The class `Dense` is the TensorFlow class encapsulating a regular fully-connected layer.\n",
    "\n",
    "Furthermore, CNNs take an even better approach towards regularization by taking advantage of the hierarchical pattern in data and assemble more complex-patterns by using smaller and simpler patterns, like Lego pieces. CNNs follow the connectivity patterns of neurons similar to the organization of the animal visual cortex. The receptive fields restrict the stimuli to clusters of neurons. These fields overlap to cover the complete visual field.\n",
    "\n",
    "CNNs require no pre-processing compared to other image classification methods because they build the features to be used for classification, which are derived/enriched/engineered features from the data. Clearly, a feature engineering is not necessary and considered as a major advantage.\n",
    "\n",
    "As a final note, RNNs track the temporal patterns and CNNs track the spatial patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## A Perceptron in TensorFlow\n",
    "\n",
    "Consider a Perceptron and the representation of it in TensorFlow framework.\n",
    "\n",
    "$y=w^{\\top} x+b$ where $y \\in \\mathbb{R}$ is the Perceptron output, $w \\in \\mathbb{R}^{M}$ are weights, $x \\in \\mathbb{R}^{M}$ is the input (a single data point with all $M$ features), and $b \\in \\mathbb{R}$ is the offset of the hyperplane defined by $w$. Then, the output can be passed to an activation function.\n",
    "\n",
    "Below code demonstrates TensorFlow 2.0,\n",
    "* a Linear SVC\n",
    "* a Perceptron\n",
    "* a 1-hidden layer neural network\n",
    "\n",
    "In TensorFlow framework, similar to PyTorch, most of the mechanics of forward propagation, back-propagation of the error, and optimization steps are hidden from the user. In fact, modeling a neural network is even simpler than PyTorch.\n",
    "\n",
    "The output of the network is mapped to a single binary value (not one hot encoded). So, we have to use `tf.keras.losses.BinaryCrossentropy()` and `tf.keras.metrics.BinaryAccuracy()` for loss and accuracy.\n",
    "\n",
    "In case we need to create the `y` values one-hot-encoded, we can convert the `y` vector: `y_bin_tr, y_bin_ts = tf.keras.utils.to_categorical(y_tr), tf.keras.utils.to_categorical(y_ts)` to be used by one-hot-encoded output.\n",
    "\n",
    "__Question:__ Why does Linear SVC warns about non-convergence for a lower value of `tol`? Why does it perform lower than expected?\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "Following table helps how to pick the correct loss function and the output layer (source: Textbook Raschka, 2019).\n",
    "\n",
    "| Loss Function  | Classification    | Example Probability Output  | Example Logit Output|\n",
    "|----------------|-------------------|----------------|-------------------|\n",
    "| `BinaryCrossEntropy`            | binary         | y_true= [1] `y_pred= [0.69]`           | y_true= [1] y_pred= [0.8] |\n",
    "| `CategoricalCrossEntropy`\t            | multi-class\t         | y_true= `[0,0,1]` y_pred= `[0.30,0.15,0.55]`             | y_true= `[0,0,1]` y_pred= `[1.5,0.8,2.1]`     |\n",
    "| `SparseCategoricalCrossEntropy`\t          | multi-class     | \ty_true= [2] `y_pred= [0.30,0.15,0.55]`          | y_true= [2] `y_pred= [1.5,0.8,2.1]`     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f'TensorFlow version= {tf.__version__}')\n",
    "print(f\"CUDA available= {tf.test.gpu_device_name()}\")\n",
    "\n",
    "# Check CUDA TensorFlow\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Locate and load the cancer data file\n",
    "bc = load_breast_cancer()\n",
    "bc_df = pd.DataFrame(data= np.c_[bc.data, [bc.target_names[v] for v in bc.target]],\n",
    "                     columns= list(bc.feature_names)+['cancer'])\n",
    "\n",
    "# Populate the dataset, cancer column is target variable\n",
    "X = bc_df.loc[:, bc_df.columns != 'cancer'].astype(np.float).values\n",
    "y = bc_df.loc[:, bc_df.columns == 'cancer'].replace({'benign':0, 'malignant':1}).values.ravel()\n",
    "\n",
    "# Sanity\n",
    "(N, M), K = X.shape, len(np.unique(y))\n",
    "print(f'#data points N= {N}, #features M= {M}, #classes K= {K}')\n",
    "\n",
    "X_tr1, X_ts1, y_tr1, y_ts1 = train_test_split(X, y, stratify=y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Build a reference classifier model\n",
    "clf = LinearSVC(random_state=0, tol=4).fit(X_tr1, y_tr1)\n",
    "print(f'Linear SVC accuracy={accuracy_score(clf.predict(X_ts1), y_ts1):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important:__ Always clear the keras session to make sure the new model starts cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the perceptron model\n",
    "nn1 = tf.keras.Sequential()\n",
    "nn1.add(tf.keras.layers.Dense(1, input_shape=(M,), activation='sigmoid'))\n",
    "\n",
    "# Sanity\n",
    "print(nn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "nn1.compile(\n",
    "    optimizer='SGD',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn1.fit(X_tr1, y_tr1, epochs=200, batch_size=64, verbose=0)\n",
    "loss, acc = nn1.evaluate(X_ts1, y_ts1, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy\n",
    "print(f'Loss= {loss:.3f}, Testing accuracy= {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity\n",
    "y_pred = nn1.predict_classes(X_ts1)\n",
    "print(' '.join([f\"{int(_):d}\" for _ in y_pred[:30]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model:__ 1-hidden layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nn2 = tf.keras.Sequential()\n",
    "nn2.add(tf.keras.layers.Dense(10, input_shape=(M,), activation='sigmoid'))\n",
    "nn2.add(tf.keras.layers.Dense(K, input_shape=(10,), activation='softmax'))\n",
    "\n",
    "# Sanity\n",
    "print(nn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "nn2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector y to one-hot encoded\n",
    "y_bin_tr, y_bin_ts = tf.keras.utils.to_categorical(y_tr1), tf.keras.utils.to_categorical(y_ts1)\n",
    "\n",
    "# Train the model\n",
    "nn2.fit(X_tr1, y_bin_tr, epochs=200, batch_size=64, verbose=0)\n",
    "loss, acc = nn2.evaluate(X_ts1, y_bin_ts, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy\n",
    "print(f'Loss= {loss:.3f}, Testing accuracy= {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity\n",
    "y_pred = nn2.predict_classes(X_ts1)\n",
    "print(' '.join([f\"{int(_):d}\" for _ in y_pred[:30]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Run the previous four cells several times, and observe that from time to time predictions turn out to be constant and the test classification drops to a low value. Change the value of the `learning_rate` to higher and lower values to see the effect.\n",
    "\n",
    "---\n",
    "\n",
    "## CNN Design\n",
    "Consider the `Conv2D` layer:\n",
    "\n",
    "tf.keras.layers.Conv2D(\n",
    "â€ƒ filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "â€ƒ dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "â€ƒ kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "â€ƒ kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "â€ƒ kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "  \n",
    "### Filters \n",
    "Consider a convolutional filter of `kernel_size=(H,W)` $H \\times W$ and a filter depth of  ð¾  (`filters=K`) where a portion of the input is mapped in 2-dimensions to a single neuron through the filter. In case there are more than 1 channel of the image, such as R, G, B then these also add as $D$ many dimensions.\n",
    "\n",
    "$z_{i, j}=\\phi\\left(b+\\sum_{l=0}^{k_{H}-1} \\sum_{m=0}^{k_{W}-1} \\sum_{n=0}^{D-1} w_{l, m, n}, x_{i+l, j+m, n}\\right)$ and $w \\in \\mathbb{R}^{k_{H} \\times k_{W} \\times D}$ is the weight of the neuron.\n",
    "\n",
    "### Strides\n",
    "The `strides` parameter is a 2-tuple of integers, specifying the step of the convolution along the x and y axis of the input volume in _pixels_.  Generally it is left as default `(1, 1)`, occasionally increase it to `(2, 2)` to help reduce the size of the output volume.\n",
    "\n",
    "### Padding\n",
    "Padding is necessary to map the input $H \\times W$ to the same dimension of $H \\times W$ at the output. $\\frac{1}{2}$ (`kernel_size`-1) many zeros are necessary on left, right, top and bottom boundaries of the input. Consequently, `kernel_size` has to be an odd number.\n",
    "\n",
    "### Subsampaling Layers\n",
    "A pooling $P$  or subsampling layer after a `Conv2D` layer reduces the spatial size of the representation to reduce the amount of parameters (i.e. weights) and computation in the network. Max-pooling layers return only the maximum value at each depth of the pooled area, and average-pooling layers compute the average at each depth of the pooled area. Pooling layer operates on each feature map independently and generally halves each image dimension, e.g. $P_{2 \\times 2}$ pooling.\n",
    "\n",
    "---\n",
    "\n",
    "## A CNN Example on MNIST Dataset\n",
    "The following example uses MNIST dataset: MNIST database http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Build a deep learning network,\n",
    "* Use  $5 \\times 5$  kernel size\n",
    "* 2 convolutional layers of filter size  16  and  32  with $P_{2 \\times 2}$  pooling between them\n",
    "* ReLU activation on CNN layers\n",
    "* Dense layer after the second pooling with ReLU activation\n",
    "* Final dense layer with the number of classes (i.e.  10) and Softmax activation\n",
    "* Input data loaded as  28 $\\times$ 28 $\\times$ 1  matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 28, 28, 1)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    #\n",
    "    return images, labels\n",
    "\n",
    "X_tr, y_tr = load_mnist('../datasets/', kind='train')\n",
    "print(f'N= {X_tr.shape[0]}, HxW= {X_tr.shape[1]}x{X_tr.shape[2]}')\n",
    "\n",
    "X_ts, y_ts = load_mnist('../datasets/', kind='t10k')\n",
    "print(f'N= {X_ts.shape[0]}, HxW= {X_ts.shape[1]}x{X_ts.shape[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our full CNN neural network\n",
    "cnn1 = tf.keras.Sequential()\n",
    "\n",
    "cnn1.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5),\n",
    "    data_format='channels_last',\n",
    "    name='conv_1', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_1'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5),\n",
    "    name='conv_2', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn1.add(tf.keras.layers.Dense(units=1024, name='fc_1', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.Dense(units=10, name='fc_2', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for repeatibility\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Build the model\n",
    "cnn1.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "# Compile the model with the optimizer, loss function and metric\n",
    "cnn1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "NUM_EPOCHS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights for debugging purposes and saving the model\n",
    "cnn1.save_weights('cnn1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = cnn1.fit(X_tr, y_tr,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing dataset\n",
    "y_pred = cnn1.predict_classes(X_ts)\n",
    "print(f'Accuracy= {sum(y_pred==y_ts)/10000:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## TensorBoard Introduction\n",
    "TensorBoard provides the visualization and tools in order to help machine learning experimentation and development:\n",
    "* Tracking and visualizing metrics such as loss and accuracy\n",
    "* Visualizing the model graph (operations and layers)\n",
    "* Viewing histograms of weights, biases, or other tensors as they change over time\n",
    "* Projecting embeddings to a lower dimensional space\n",
    "* Displaying images, text, and audio data\n",
    "* Profiling TensorFlow programs\n",
    "\n",
    "More about TensorBoard: https://www.tensorflow.org/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '.\\\\logs\\\\fit\\\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Clear the session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Our previous neural network\n",
    "nn2 = tf.keras.Sequential()\n",
    "nn2.add(tf.keras.layers.Dense(10, input_shape=(M,), activation='sigmoid', name='fc_1'))\n",
    "nn2.add(tf.keras.layers.Dense(K, input_shape=(10,), activation='softmax', name='fc_2'))\n",
    "\n",
    "nn2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = nn2.fit(X_tr1, y_bin_tr, epochs=200, batch_size=64, verbose=0,\n",
    "                  validation_data=(X_tr1, y_bin_tr),\n",
    "                  callbacks=[tensorboard_callback])\n",
    "loss, acc = nn2.evaluate(X_ts1, y_bin_ts, verbose=0)\n",
    "\n",
    "# Sanity\n",
    "print(f'Loss= {loss:.3f}, Testing accuracy= {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Important:__ In a new Anaconda Prompt, go to the notebook folder and run `tensorboard --logdir logs/fit` after activating `tf` virtual environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard within the notebook using magics\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T14:14:10.511942Z",
     "iopub.status.busy": "2020-08-20T14:14:10.511734Z",
     "iopub.status.idle": "2020-08-20T14:14:10.634321Z",
     "shell.execute_reply": "2020-08-20T14:14:10.633602Z",
     "shell.execute_reply.started": "2020-08-20T14:14:10.511920Z"
    }
   },
   "source": [
    "## CNN TensorBoard Demonstration\n",
    "![title](img/tensorboard_demo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "# This info is stored under C:\\Users\\guvene1\\AppData\\Local\\Temp\\.tensorboard-info\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Run the CNN above with proper validation dataset pulled from training portion of the MNIST Dataset. Create the `fit` history and display on TensorBoard.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
