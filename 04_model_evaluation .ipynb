{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:06:33.582905Z",
     "start_time": "2020-08-06T13:06:33.579958Z"
    }
   },
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the machine learning model and inherently the development pipeline is an essential part of any *__learning__* project. There are two perspectives:\n",
    "\n",
    "* Verifying and validating the learning approach/pipeline/model\n",
    "* Deploying the ML model\n",
    "\n",
    "Verifying the model always involves a training and testing dataset and frequently a validation dataset.\n",
    "\n",
    "1. The training, testing, and validation datasets are drawn from the same main dataset. Thus any underlying probabilistic distribution of the collected and preprocessed dataset is assumed to apply to all these three pieces.\n",
    "2. Training dataset is used to develop the model. Training dataset is generally non-overlapping, or in other words, samples are drawn __without replacement__.\n",
    "3. Validation dataset is used to __fine-tune__ the model. Grid search model parameters and pick the best performance based on the __validation performance only__. Then the testing dataset is used to actually measure the __trained__ and __tuned__ model. If a model does not need a validation dataset (i.e. no parameters to tune, such as Naive Bayes), then we can add the validation dataset to the training dataset.\n",
    "4. Testing dataset is used to verify and evaluate the model performance.\n",
    "\n",
    "**Important:** Though evaluation is about measuring the performance, it is also setting the details of the model development pipeline in such a way that the exact pipeline will also be used for the **product** development or deployment.\n",
    "\n",
    "Note that through model evaluation we are making sure that what we have done as model development is correct. If there is a next project step where we **deploy** the model in a computer system (in fact it would be our actual product), so that our developed model would actually predict something, then the most common approach is using the entire dataset to train and develop the model, exactly as the evaluation pipeline is constructed. Then a possible overall approach would be use train/test/validate to build and verify the pipeline, and compute some metrics so that we can scholarly show that the developed model actually learned something and performs better than a **random** classifier.\n",
    "\n",
    "**Question:** If there are 5 target classes, then what is the lower-bound accuracy a random classifier would achieve?\n",
    "\n",
    "Model evaluation answers a few questions:\n",
    "\n",
    "* Classification - What is the expected accuracy a model would achieve when given unseen data points and asked to predict?\n",
    "* Classification - What is the expected accuracy of individual classes and confusion between them (i.e. when the class A is truth for a particular data point, is class B is predicted, or class C is predicted?)\n",
    "* Regression - What is the total error between the regression curve (simplified model, fitted curve, etc.) and the actual data points?\n",
    "* If we are OK with a particular __False-Alarm-Rate__, then what is the highest accuracy we can achieve? For example, based on Receiver Operating Characteristic (__ROC__) curve what should be our operating point.\n",
    "* What is the variation of the classification error?\n",
    "* Can we estimate the model generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Methods\n",
    "**Training-testing-validation:** Common approach. In our project, we have to come up with a suitable training-testing-validation dataset. A good example can be as in the following: Given 20 year long dataset where we know the data is collected every day for 20 years, the training dataset can use all those 20 years. The validation dataset can use all 20 years but a percentage portion of the training (e.g. 10%). The testing dataset can be drawn from the last 5 years. Assuming the unseen data would be somewhat closer to the recent years. This kind of decisions are to be made by the model developer working with the subject matter experts.\n",
    "\n",
    "**K-fold cross validation:** Common approach. Parameter tuning is not done but parameters are fixed throughout the entire development and evaluation. Each fold has non-overlapping test and train datasets. Average the computed accuracies for each fold and report as the accuracy. Clearly, a ML method with **good generalization** and rather **insensitive to model parameters** would work much better with this evaluation model.\n",
    "\n",
    "**Leave-one-out-cross-validation (LOOCV):** Seldom used. Similar to k-fold cross validation but testing dataset is always a single point. Recommended for very small datasets or when several outlier data points exist in the set and they are to be learned by the model.\n",
    "\n",
    "**Stratification:** When target categories are unbalanced (e.g. there are more benign cases rather than malignant), then a correct validation requires the training/test datasets, or folds, preserving the percentage of samples for each class. `sklearn` library supports stratification fully, such as by the use of `StratifiedKFold` library function.\n",
    "\n",
    "The **most important aspect** of model evaluation is measuring the **expected** real-world performance with a good generalization.\n",
    "\n",
    "## Generalization\n",
    "The developed model's ability to adapt (not a real adaptation but more like being ready) properly to new, previously unseen data, drawn from the same probabilistic distribution as the one used to create the model. Some classifiers which have good generalization ability:\n",
    "* Support Vector Machines (large-margin) classifiers, since they solve an optimization problem to maximize the distance between data points and the decision (discrimination) surface.\n",
    "* Random Forest (ensemble) classifiers, since they use few features per classifier, but numerous simple classifiers which randomly employ features, and then take the majority of the predictions made by this ensemble of classifiers.\n",
    "* Principal Component Analysis (PCA) for unsupervised learning, since they try to find simplified data representation by looking at the largest eigenvalues of the covariance matrix of features.\n",
    "* Human beings üòÉ\n",
    "\n",
    "## Evaluation Metrics\n",
    "Following is a list of evaluation metrics.\n",
    "\n",
    "__1. Classification Accuracy__  \n",
    "This the major metric we will use.\n",
    "\n",
    "Accuracy $=\\frac{\\text { number of correct predictions }}{\\text { total number of predictions }}$\n",
    "\n",
    "__2. Confusion Matrix__  \n",
    "This is the second major metric and it is defined for binary classification.\n",
    "\n",
    "| __N=33__    | __truth A__ | __truth B__ |\n",
    "|-------------|-------------|-------------|\n",
    "| predicted A |          10 |           3 |\n",
    "| predicted A |           1 |          20 |\n",
    "\n",
    "\n",
    "Historically from statistics, a positive and a negative class is defined, such as patient has cancer is `positive`, and does not have is `negative`. Assume class A is positive and class B is negative.\n",
    "\n",
    "__Type I error__ - missing the positive, mis-predict class A - patient has cancer and missed.\n",
    "__Type II error__ - patient does not have cancer but misdiagnosed - False Alarm.\n",
    "\n",
    "Type I errors are very costly.\n",
    "\n",
    "True Positive (TP) = Predicted A, truth is A  \n",
    "True Negative (TN) = Predicted B, truth is B  \n",
    "False Positive (FP) = Predicted A, truth is B - Type II error  \n",
    "False Negative (FN) = Predicted B, truth is A - Type I error - major mistake\n",
    "\n",
    "---\n",
    "\n",
    "__Example:__ TP=10, TN=20, FP=3, FN=1\n",
    "\n",
    "Accuracy $=\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{TN}+\\mathrm{FP}+\\mathrm{FN}}=\\frac{10+20}{10+20+3+1}$\n",
    "\n",
    "TP Rate $(\\mathrm{TPR})=\\frac{\\mathrm{TP}}{\\sum \\text { truth positive }}=\\frac{10}{10+1}$\n",
    "\n",
    "TN Rate $(\\mathrm{TNR})=\\frac{\\mathrm{TN}}{\\sum \\text { truth negative }}=\\frac{20}{20+3}$\n",
    "\n",
    "---\n",
    "\n",
    "Also from Information Retrieval, following metrics are defined,  \n",
    "Precision $=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$ How many selected items are relevant?  \n",
    "Recall $=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$ How many relevant items are selected? \n",
    "\n",
    "__3. F1-score__  \n",
    "Also known as harmonic mean of precision and recall.  \n",
    "$\\mathrm{F}-1$ score $=2 \\frac{\\text { Precision } \\times \\text { Recall }}{\\text { Precision }+\\text { Recall }}$  \n",
    "F score attempts to find a balance between precision and recall\n",
    "\n",
    "__4. Area Under Curve (AUC)__  \n",
    "Also known as area under Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    "It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "__5. Mean Absolute Error__  \n",
    "(MAE) is the average error between the original  ùë•ùëñ  and the predicted  ùë•ÃÇ ùëñ  values for regression problems.\n",
    "\n",
    "$\\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|x_{i}-\\hat{x}_{i}\\right|$\n",
    "\n",
    "__6. Mean Squared Error (MSE)__  \n",
    "MSE is similar to MAE and is more common than MAE.\n",
    "\n",
    "$\\mathrm{MSE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\hat{x}_{i}\\right)^{2}$\n",
    "\n",
    "__7. Logarithmic Loss__\n",
    "Also known as Log Loss measures false classifications and suitable for multi-class classification (rather less common error metric).\n",
    "\n",
    "$\\operatorname{LogLoss}=\\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{i j} \\log \\left(p_{i j}\\right)$\n",
    "\n",
    "$y_{ij}$ indicates whether sample $i$ belongs to class $j$ or not  \n",
    "$p_{ij}$ indicates the probability (or score) of sample $i$ belonging to class $j$  \n",
    "\n",
    "LogLoss has no upper bound and it exists on the range  $[0,\\infty)$. Log Loss measure close to 0 indicates a higher accuracy. In general, minimizing LogLoss gives greater accuracy for the classifier.\n",
    "\n",
    "## Receiver Operating Characteristic Curve\n",
    "The __ROC__ curve is composed of true positive rate (__TPR__) on the y-axis and the false positive rate (__FPR__) on the x-axis while each operating point corresponds to some detection threshold or a classifier model parameter.\n",
    "\n",
    "The best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing zero false negatives and zero false positives, and perfect prediction. A random guess or a coin flip would result a point along the diagonal line (also called line of no-discrimination) from the left bottom to the top right corners (regardless of the positive and negative base rates).\n",
    "\n",
    "ROC can be used for both model evaluation (such as the area under the ROC curve, presents the model behavior with different parameters) and, or deciding on a model parameter for deployment, i.e. setting the TPR with a given __accepted false alarm rate__. ROC is used to make a model selection or set an operating classifier threshold by considering the TPR-FPR together. Generally, ROC is used for binary classification.\n",
    "\n",
    "An example ROC curve is generated in cells below using breast cancer data from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:27.465246Z",
     "start_time": "2020-08-12T21:45:26.679016Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Locate and load the data file\n",
    "bc = load_breast_cancer()\n",
    "bc_df = pd.DataFrame(data= np.c_[bc.data, [bc.target_names[v] for v in bc.target]],\n",
    "                     columns= list(bc.feature_names)+['cancer'])\n",
    "# See how the data looks like\n",
    "bc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:27.470732Z",
     "start_time": "2020-08-12T21:45:27.467084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Populate the dataset, cancer column is target variable\n",
    "X = bc_df.loc[:, bc_df.columns != 'cancer'].values\n",
    "y = bc_df.loc[:, bc_df.columns == 'cancer'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:27.581146Z",
     "start_time": "2020-08-12T21:45:27.472563Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Display OP\n",
    "def annot(opi, x, y):\n",
    "    plt.annotate(f\"OP{opi}\", xy=(x, y), xytext=(.90*x+.1, .80*y), arrowprops=dict(facecolor='lightgray', shrink=1))\n",
    "\n",
    "# Training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.98, random_state=14)\n",
    "\n",
    "# Parameter to vary for Logistic Regression\n",
    "C = (2e-1, 0.5, 0.8, 1, 2, 5, 1e1, 2e1, 1e2)\n",
    "\n",
    "# Let's vary C and generate training/testing sessions to collect data for ROC\n",
    "FPR, TPR = [], []\n",
    "for c in C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14,\n",
    "                                               penalty='l1',\n",
    "                                               solver='liblinear',\n",
    "                                               class_weight='balanced',\n",
    "                                               C=c,\n",
    "                                               multi_class='auto',\n",
    "                                               max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    y_pred = pipe_lr.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]  # Pd\n",
    "    FPR += [fp/(fp+tn)]  # Pf\n",
    "    #\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:27.835176Z",
     "start_time": "2020-08-12T21:45:27.583553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sorts the points to display nicely on ROC\n",
    "FPR, TPR = zip(*sorted(zip(FPR, TPR)))\n",
    "fpr = [0.]+list(FPR)+[1.]; tpr = [0.]+list(TPR)+[1.]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(fpr, tpr, ':', label='ROC')\n",
    "plt.scatter(FPR, TPR, 50, color='red', marker='o', label='operating points')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='coin flip')\n",
    "\n",
    "# Annotate certain operating points\n",
    "annot(1, fpr[1], tpr[1])\n",
    "annot(2, fpr[4], tpr[4])\n",
    "annot(3, fpr[8], tpr[8])\n",
    "annot(4, fpr[9], tpr[9])\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dramatization:** Note that in order to generate a nice ROC for __demonstration purposes__, we selected a harsh test size of 98% and `random_state=14`. Also picked `LogisticRegression` with parameters `penalty='l1'`, `solver='liblinear'`. Not every classifier generates an ROC curve like above due to non-existence of a parameter that can vary detection versus false alarm smoothly.\n",
    "\n",
    "**Important Question:** Given above ROC, which operating point would you pick for cancer detection? OP1, OP2, OP3, or OP4?\n",
    "\n",
    "---\n",
    "\n",
    "## Generalization Error\n",
    "From statistical learning theory, the __generalization error__ is the difference between the expected and empirical error. Or, the difference between error on the training set and error on the underlying joint probability distribution.\n",
    "\n",
    "__Question:__ If we know the underlying joint probability distribution of the data, then would we need an ML method?\n",
    "\n",
    "__Answer:__ No. We will probably never be able to know the underlying data probability distribution for practical ML problems.\n",
    "\n",
    "---\n",
    "\n",
    "Student: Below cell can be safely ignored. This is the code to display tables left oriented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
