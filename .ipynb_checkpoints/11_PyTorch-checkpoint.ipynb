{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T23:43:17.557011Z",
     "iopub.status.busy": "2020-08-19T23:43:17.556846Z",
     "iopub.status.idle": "2020-08-19T23:43:17.560486Z",
     "shell.execute_reply": "2020-08-19T23:43:17.559669Z",
     "shell.execute_reply.started": "2020-08-19T23:43:17.556993Z"
    }
   },
   "source": [
    "# Introduction to PyTorch\n",
    "## Introduction\n",
    "__PyTorch__ is a popular Python based computational graph framework to implement deep learning algorithms.\n",
    "\n",
    "The sequence analysis or temporal information can be captured by a special class of artificial neural networks, called recurrent neural network (RNN), where connections between nodes form a directed graph along a sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected speech recognition and natural language processing. PyTorch stands out among other neural network frameworks by its simpler programming and RNN capabilities.\n",
    "\n",
    "The general machine learning pipelines can be seen as a data flow architecture where the input vectors are transformed by the model (a mathematical expression) to obtain predictions, and the loss function (e.g. the error difference between the intermediate model output and the actual desired output) to provide a feedback signal to adjust the parameters of the model. This data flow can be conveniently implemented using the computational graph data structure.\n",
    "\n",
    "Technically, a computational graph is an abstraction that models mathematical expressions.\n",
    "\n",
    "Consider the expression: $y=w x+b$\n",
    "\n",
    "This equation can be written as two subexpressions, $z=w x$ and $y=z+b$.\n",
    "\n",
    "We can then represent the original expression using a Directed Acyclic Graph (DAG) in which the nodes are the mathematical operators, e.g. multiplication and addition. The inputs to the operations are the incoming graph edges to the nodes and the output of each operation is the outgoing edge.\n",
    "\n",
    "## Basics\n",
    "At the core of the PyTorch library is the tensor $\\mathbf{T}$, which is a mathematical object holding multidimensional data.\n",
    "\n",
    "A tensor of order zero is a scalar (e.g. a `real` number).\n",
    "\n",
    "A tensor of order one ($1^{\\text {st}}$ order tensor) is a one-dimensional vector or an array of $M$ many numbers, i.e. $\\mathbf{T} \\in \\mathbb{R}^{M}$.\n",
    "\n",
    "Similarly, a $2^{\\mathrm{nd}}$ order tensor is an array of vectors, or an $M \\times M$ matrix, i.e. $\\mathbf{T} \\in \\mathbb{R}^{M \\times M}$.\n",
    "\n",
    "Therefore, a tensor can be generalized as an n-dimensional array of scalars, i.e. n-dimensional tensor = $\\mathbf{T} \\in \\mathbb{R}^{M \\times M \\times \\ldots \\times M}$\n",
    "\n",
    "Note that, for simplicity each data size is set to $M$ above, but this can be easily changed to any value in PyTorch framework, similar to `numpy` multidimensional arrays.\n",
    "\n",
    "---\n",
    "\n",
    "Use a separate Anaconda environment for PyTorch:  \n",
    "`conda create -n torch pytorch jupyter matplotlib pandas scikit-learn`  \n",
    "`conda activate torch`  \n",
    "\n",
    "Run this notebook under the virtual environment `torch`.\n",
    "\n",
    "---\n",
    "\n",
    "PyTorch API: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T12:22:47.946954Z",
     "iopub.status.busy": "2020-08-20T12:22:47.946695Z",
     "iopub.status.idle": "2020-08-20T12:22:48.531114Z",
     "shell.execute_reply": "2020-08-20T12:22:48.530305Z",
     "shell.execute_reply.started": "2020-08-20T12:22:47.946925Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-daa9a4f152a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'PyTorch version= {torch.__version__}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch version= {torch.__version__}')\n",
    "print(f'CUDA available= {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to display properties of the Python objects\n",
    "def describe(x):\n",
    "    print(f\"Type= {x.type()}\")\n",
    "    print(f\"Shape/size= {x.shape}\")\n",
    "    print(f\"Values= {x}\")\n",
    "\n",
    "# Random tensor, has dummy values from the computer memory - not initialized\n",
    "describe(torch.Tensor(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 3)\n",
    "describe(x)\n",
    "\n",
    "# Initialization\n",
    "x.fill_(5)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize from a numpy array\n",
    "np_array = np.random.rand(2, 3)\n",
    "describe(torch.from_numpy(np_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization using arange\n",
    "x = torch.arange(6)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert shape (1,6) to shape (2,3)\n",
    "x = x.view(2, 3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum values on dimension 0, i.e. vertically on columns\n",
    "describe(torch.sum(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum values on dimension 1, i.e. horizontally on rows\n",
    "describe(torch.sum(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose of the matrix\n",
    "describe(torch.transpose(x, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing works similarly to numpy\n",
    "describe(x)\n",
    "describe(x[:1, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computational Graphs\n",
    "PyTorch tensor class encapsulates the data (the tensor itself) and a range of operations, such as algebraic operations, indexing, and reshaping operations. When the `requires_grad` Boolean flag is set to True on a tensor, bookkeeping operations are enabled that can track the gradient at the tensor as well as the gradient function, both of which are needed to facilitate the gradient based deep learning. Recall that a neural network is _trained_ by minimizing the error between the output and the target output values by an optimization process where generally a gradient based approach is used since the optimization problem is __non-convex__ and require a greedy optimization approach.\n",
    "\n",
    "__Important:__ Running the following cells multiple times will fail as the intermediate gradients are computed with the current data and consecutive runs will receive no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2 by 2 matrix, all ones\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "describe(x)\n",
    "\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear computation\n",
    "y = (x+2) * (x+5) + 3\n",
    "describe(y)\n",
    "\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take mean of y, gradient is none\n",
    "z = y.mean()\n",
    "describe(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a tensor with `requires_grad=True` is created, PyTorch manages the bookkeeping that computes the gradients. First, PyTorch will keep track of the values of the forward pass. Then, at the end of the computations, a single scalar is used to compute a backward pass. The backward pass is initiated by using the `backward()` method on a tensor resulting from the evaluation of a __loss function__. The backward pass computes a gradient value for a tensor object that participated in the forward pass.\n",
    "\n",
    "In general, the gradient is a value that represents the slope of a function output with respect to the function input. In the computational graph, gradients exist for each model parameter and can be thought of as the parameter's contribution to the error signal. The gradients for the nodes in the computational graph are accessible by using the `.grad` member variable. Optimizers use the `.grad` variable to update the values of the parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptron Re-visited\n",
    "Each perceptron has an input $x$, an output $y$, and a set of weights $w$, a bias $b$, and an activation function $f$. The weights and the bias are __learned__ from the data, and the activation function is handpicked depending on the neural network model designer's intuition of the network and its target outputs.\n",
    "\n",
    "$y=f(w x+b)$\n",
    "\n",
    "Generally more than one feature is used, thus $x \\in \\mathbb{R}^{M}$, $w \\in \\mathbb{R}^{M}$, and $b \\in \\mathbb{R}$, where there are $M$ features.\n",
    "\n",
    "The activation function, denoted by f is typically a nonlinear function. Thus, a perceptron is a composition of a linear and a nonlinear function. The linear expression $w x+b$ is also known as the __affine transformation__ which preserves points, straight lines and planes. Some affine transformations include translation, scaling, reflection, rotation, etc., and compositions of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T12:43:04.086409Z",
     "iopub.status.busy": "2020-08-20T12:43:04.086134Z",
     "iopub.status.idle": "2020-08-20T12:43:04.098154Z",
     "shell.execute_reply": "2020-08-20T12:43:04.097139Z",
     "shell.execute_reply.started": "2020-08-20T12:43:04.086377Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-781f713698a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\" A perceptron is one linear layer \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" A perceptron is one linear layer \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): size of the input features\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\" The forward pass of the perceptron\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "            x_in.shape should be (batch, num_features)\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "Activation functions are nonlinear decision making stages to make sure the neural network can indeed approximate a function that models the input and output relation.\n",
    "\n",
    "### Sigmoid\n",
    "sigmoid $:(-\\infty, \\infty) \\rightarrow[0,1] \\quad \\operatorname{sigmoid}(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "### Tanh\n",
    "$\\tanh :(-\\infty, \\infty) \\rightarrow[-1,1] \\quad \\tanh (x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n",
    "\n",
    "### ReLU\n",
    "The most popular activation function also called as rectified linear unit.\n",
    "\n",
    "relu $:(-\\infty, \\infty) \\rightarrow[0, \\infty) \\quad \\operatorname{relu}(x)=\\max (0, x)$\n",
    "\n",
    "The clipping effect of ReLU that helps with the vanishing gradient problem can also become an issue where over time certain outputs in the network can simply become zero and never revive again. This is called the \"dying ReLU\" problem. To mitigate that effect, variants such as the Leaky ReLU and Parametric ReLU (PReLU) activation functions have proposed, where the leak coefficient $a$ is a learned parameter.\n",
    "\n",
    "PReLU: $f(x)=\\max (a x, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-5, 5, .1)\n",
    "y1 = torch.sigmoid(x)\n",
    "y2 = torch.tanh(x)\n",
    "y3 = torch.relu(x)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "ax=plt.subplot(1, 3, 1); ax.grid(); plt.title(\"sigmoid\", x=0.2, y=0.8)\n",
    "ax.plot(x.numpy(), y1.numpy())\n",
    "\n",
    "ax=plt.subplot(1, 3, 2); ax.grid(); plt.title(\"tanh\", x=0.2, y=0.8)\n",
    "ax.plot(x.numpy(), y2.numpy())\n",
    "\n",
    "ax=plt.subplot(1, 3, 3); ax.grid(); plt.title(\"ReLU\", x=0.2, y=0.8)\n",
    "ax.plot(x.numpy(), y3.numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T12:48:48.200903Z",
     "iopub.status.busy": "2020-08-20T12:48:48.200649Z",
     "iopub.status.idle": "2020-08-20T12:48:48.204037Z",
     "shell.execute_reply": "2020-08-20T12:48:48.203094Z",
     "shell.execute_reply.started": "2020-08-20T12:48:48.200876Z"
    }
   },
   "source": [
    "## Softmax\n",
    "Like the sigmoid function, the softmax function squashes the output of each unit to be between 0 and 1. In addition, the softmax also divides each output by the sum of all the outputs, which gives us a discrete probability distribution over $K$ possible classes. The values in the resulting distribution all sum up to one, like a probability.\n",
    "\n",
    "$\\operatorname{softmax}\\left(x_{i}\\right)=\\frac{e^{x_{i}}}{\\sum_{j=1}^{K} e^{x_{j}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the softmax output \n",
    "softmax = nn.Softmax(dim=1)\n",
    "x_input = torch.randn(1, 3)\n",
    "y_output = softmax(x_input)\n",
    "\n",
    "print(x_input)\n",
    "print(y_output)\n",
    "print(torch.sum(y_output, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loss Functions\n",
    "Another piece we require to build learning computation graphs is the loss functions or objective functions. The training algorithm picks the correct (fitting) parameters by evaluating the loss function.\n",
    "\n",
    "Recall that we have used a loss function as the sum of difference between the truth $y$ and the (intermediate) prediction $\\hat{y}$. The higher this difference, the worse the model prediction.\n",
    "\n",
    "### Mean Squared Error Loss (L2)\n",
    "Similar to linear regression, the network's output $\\hat{y}$ and the target $y$ are real valued number and their difference is used as the loss/error/cost:\n",
    "\n",
    "$\\mathbf{L}_{\\mathrm{MSE}}=\\frac{1}{n} \\sum_{i=1}^{n}(y-\\hat{y})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MSE loss\n",
    "mse_loss = nn.MSELoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.randn(3, 5)\n",
    "\n",
    "loss = mse_loss(outputs, targets)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss\n",
    "The categorical cross-entropy (CCE) loss is typically used in a multi-class classification problem in which the outputs are interpreted as predictions of class membership probabilities. The target $y$ is a vector of $K$ elements that represents the true multinomial distribution over all the classes. If only one class is correct or permitted, then this vector is a one-hot vector. The network's output $\\hat{y}$ is also a vector of $K$ elements and represents the network's prediction of the multinomial distribution.\n",
    "\n",
    "$\\mathbf{L}_{\\text {cross-entropy }}(y, \\hat{y})=-\\sum_{c=1}^{K} y_{c} \\log \\left(\\hat{y}_{c}\\right)$\n",
    "\n",
    "Best to use categorical cross-entropy in classification problems where only one result can be correct.\n",
    "\n",
    "## Binary Cross-Entropy Loss\n",
    "The binary cross-entropy (BCE) is the binary version of the CCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CCE loss\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64)\n",
    "\n",
    "loss = ce_loss(outputs, targets)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate BCE loss\n",
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\n",
    "\n",
    "loss = bce_loss(probabilities, targets)\n",
    "\n",
    "print(probabilities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Supervised Learning with PyTorch\n",
    "Once the network is constructed, and loss function is picked, an optimizer is chosen which can update the model parameters and fit to the training data.\n",
    "\n",
    "A single `hyperparameter` that controls the network parameter update behavior of the optimizer is called the learning rate `eta`, controls how much impact the error signal has on updating the weights.\n",
    "\n",
    "Two popular optimizers in PyTorch are Stochastic Gradient Descent (SGD), and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Adam optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 2\n",
    "learning_rate = 0.001  # this must be found by a hyper-parameter search\n",
    "\n",
    "perceptron = Perceptron(input_dim=input_dim)  # defined above\n",
    "bce_loss = nn.BCELoss()  # Binary cross entropy loss\n",
    "optimizer = optim.Adam(params=perceptron.parameters(), lr=learning_rate)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In batches, by a particular learning rate, for so many epochs:  \n",
    "1\\. Zero the gradients  \n",
    "2\\. Compute the output  \n",
    "3\\. Compute the loss\n",
    "4\\. Use loss to produce gradients\n",
    "5\\. Use optimizer to take gradient step\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "Using Tf-Idf features let's see how we would classify programming languages.\n",
    "\n",
    "The dataset is composed of Stackoverflow website (https://stackoverflow.com/) posts about 20 different programming languages.\n",
    "\n",
    "We will use 1000 features to keep the classifier model small, and still have very good results.\n",
    "\n",
    "First let's see the SVM performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('../datasets/module05_stack_overflow_data.csv')\n",
    "df = df[pd.notnull(df['tags'])]\n",
    "print(f\"Number of words= {df['post'].apply(lambda x: len(x.split(' '))).sum()}\")\n",
    "\n",
    "plCats = np.unique(df['tags'])\n",
    "print(f\"K categories={len(plCats):d} {plCats}\")\n",
    "\n",
    "X_tfidf = TfidfVectorizer(dtype=np.float32, max_features=1000).fit_transform(df.post).todense()\n",
    "print(f\"N data points= {X_tfidf.shape[0]}, M features= {X_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_eval_docs(_clf, _Xdocs, _ydocs):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Need indexable data structure\n",
    "    acc = []\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "    for train_index, test_index in kf.split(_Xdocs, _ydocs):\n",
    "        _clf.fit(_Xdocs[train_index], _ydocs[train_index])\n",
    "        y_pred = _clf.predict(_Xdocs[test_index])\n",
    "        acc += [accuracy_score(_ydocs[test_index], y_pred)]\n",
    "    return np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "acc = kfold_eval_docs(LinearSVC(class_weight='balanced'), X_tfidf, df.tags)\n",
    "\n",
    "print(f\"Linear SVM 10-fold CV accuracy= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the neural network classifier:\n",
    "* In our neural network, let's have 1 hidden layer of size  100\n",
    "* Note that our input size $M$ is  1000 which are the columns of the Tf-Idf matrix or top words (i.e. features)\n",
    "* And we have 20 categories as output/target in the programming languages dataset\n",
    "* Let's also use ReLU activation function and a softmax output function\n",
    "* Use a CCE loss function, `CrossEntropyLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    \"\"\" A PyTorch neural network model \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 100)  # A simple input layer\n",
    "        self.fc2 = nn.Linear( 100, 100)  # A simple hidden layer\n",
    "        self.fc3 = nn.Linear( 100,  20)  # A simple output layer\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        if apply_softmax:\n",
    "            x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural network\n",
    "Net1 = MyNetwork()\n",
    "print(Net1)\n",
    "\n",
    "# Set the learning rate - this part is magic of course\n",
    "eta = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the optimizer as Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Stochastic Gradient Descent optimizer\n",
    "optimizer = optim.SGD(Net1.parameters(), lr=eta, momentum=0.9)\n",
    "\n",
    "# Create the loss function\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data arrays as Tensors.\n",
    "\n",
    "__Important:__ PyTorch requires class labels (integers) being int64, e.g. the y vector.\n",
    "\n",
    "Also use a validation dataset to measure generalization performance (pulled as 10% of training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Class labels (integers) are int64 for PyTorch\n",
    "y_tfidf = np.array(df.tags.astype('category').cat.codes, dtype='int64')\n",
    "\n",
    "# Use validation to measure generalization performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_tfidf, test_size=0.3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "X_val, y_val = torch.tensor(X_val), torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prediction function which returns the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(_x):\n",
    "    net_out = Net1.forward(_x, apply_softmax=True)\n",
    "    p_values, indices = net_out.max(dim=1)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop as in the following.\n",
    "\n",
    "Note that `minibatch_size` is also to be determined by hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "epochs=100\n",
    "minibatch_size=1000\n",
    "\n",
    "# The main training loop\n",
    "for i in range(epochs):\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        # step 1.\n",
    "        optimizer.zero_grad()\n",
    "        # step 2.\n",
    "        net_out = Net1.forward(X_train[batch_idx])\n",
    "        # step 3.\n",
    "        loss = loss_func(net_out, y_train[batch_idx])\n",
    "        # step 4.\n",
    "        loss.backward()\n",
    "        # step 5.\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred = predict(X_train)\n",
    "    y_val_pred = predict(X_val)\n",
    "    \n",
    "    train_acc = (torch.sum(y_train == y_pred).float() / X_train.shape[0])\n",
    "    val_acc = (torch.sum(y_val == y_val_pred).float() / X_val.shape[0])\n",
    "    \n",
    "    sys.stderr.write(f\"\\r{i+1}/{epochs} | Cost: {loss:.2f} | Train/Valid Acc.: {train_acc*100:.2f}%/{val_acc*100:.2f}%\")\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance on the testing dataset that was set aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test)\n",
    "test_acc = (torch.sum(y_test == y_pred).float() / X_test.shape[0])\n",
    "\n",
    "print(f\"Feedforward NN testing accuracy= {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the entire PyTorch based feedforward neural network as a Python class and measure it's 10-fold CV performance like we did for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    \"\"\" A PyTorch neural network model \"\"\"\n",
    "    def __init__(self, n_hidden=30, epochs=100, eta=0.05, minibatch_size=50):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "        self.fc1, self.fc2, self.fc3 = None, None, None\n",
    "\n",
    "    def _forward(self, X, apply_softmax=False):\n",
    "        assert self.fc1 != None\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        if apply_softmax:\n",
    "            X = F.softmax(X, dim=1)\n",
    "        return X\n",
    "\n",
    "    def _reset(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                 m.reset_parameters()\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert self.fc1 != None\n",
    "        net_out = self._forward(X, apply_softmax=True)\n",
    "        p_values, indices = net_out.max(dim=1)\n",
    "        return indices\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self._reset()  # Reset the neural network weights\n",
    "        n_output= np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features= X_train.shape[1]\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_features, self.n_hidden)  # A simple input layer\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_hidden)  # A simple hidden layer\n",
    "        self.fc3 = nn.Linear(self.n_hidden, n_output)  # A simple output layer\n",
    "        \n",
    "        optimizer = optim.SGD(self.parameters(), lr=self.eta, momentum=0.9)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                net_out = self._forward(X_train[batch_idx])\n",
    "                \n",
    "                loss = loss_func(net_out, y_train[batch_idx])\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = CustomMLP(100, 100, 0.05, 1000)\n",
    "\n",
    "acc = kfold_eval_docs(clf, torch.tensor(X_tfidf), torch.tensor(y_tfidf))\n",
    "\n",
    "print(f\"PyTorch Feedforward NN 10-fold CV accuracy= {np.mean(acc):.2f} {chr(177)}{np.std(acc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Recurrent Neural Network\n",
    "A simple RNN can be built by feeding back the output to the input through a __hidden__ layer.\n",
    "\n",
    "Consider 2 linear layers which operate on an input and a hidden state, with a `LogSoftmax` layer after the output as in the figure. The training is simply feeding the signal tensor together with the input and the previous hidden output to feed the network. Since the signal is composed of a stream of features, such as  $N^{(i)}$ number of data points and $M$ features, we will train the network with every signal $i$ with the hidden state and resetting the hidden state between signals. The hidden state will hold the temporal knowledge the signal carries. Thus, the `train` below is running the $N$ many data points of the signal `sxx`. The label `y` has a single value corresponds to the class of `sxx`. The label `y` has a single value corresponds to the class of `sxx`.\n",
    "\n",
    "Other more advanced RNN layers `nn.LSTM` and `nn.GRU` would improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output, eta=0.0005):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.i2h = nn.Linear(n_features + n_hidden, n_hidden)\n",
    "        self.i2o = nn.Linear(n_features + n_hidden, n_output)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.eta = eta  # learning rate\n",
    "        \n",
    "        # loss function, since the last layer is nn.LogSoftmax\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), dim=1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.n_hidden)\n",
    "\n",
    "    def train(self, sxx, y):\n",
    "        hidden = self.init_hidden()\n",
    "        self.zero_grad()\n",
    "\n",
    "        N = sxx.shape[0]\n",
    "        for i in range(N):\n",
    "            output, hidden = self.forward(sxx[i].reshape(1,self.n_features), hidden)\n",
    "\n",
    "        loss = self.criterion(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.data.add_(-self.eta, p.grad.data)\n",
    "\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T13:16:15.569156Z",
     "iopub.status.busy": "2020-08-20T13:16:15.568887Z",
     "iopub.status.idle": "2020-08-20T13:16:15.573709Z",
     "shell.execute_reply": "2020-08-20T13:16:15.572360Z",
     "shell.execute_reply.started": "2020-08-20T13:16:15.569125Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
