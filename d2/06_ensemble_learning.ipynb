{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T13:59:28.560174Z",
     "start_time": "2020-08-13T13:59:28.556379Z"
    }
   },
   "source": [
    "# Ensemble Learning\n",
    "## Introduction\n",
    "Ensemble methods combine multiple hypotheses, aiming to form a better one than the best hypothesis alone. Often, ensemble methods use multiple weak learners to build a strong learner.\n",
    "\n",
    "A **weak learning** is one that consistently generates better predictions than random.\n",
    "\n",
    "Ensemble methods use multiple learning algorithms (or instances) to obtain better predictive performance than could be obtained from any of the individual learning algorithms (or instances) alone. A machine learning ensemble can consist of a finite set of alternative models, which are more general classification models discretely.\n",
    "\n",
    "The common ensemble training scheme employs numerous weaker classifiers that use only subsets of the dataset in terms of rows (data points) or columns (features), and then conducts a majority voting among their resulting predictions to decide on a particular single data point classification.\n",
    "\n",
    "One important advantage of ensemble methods is the reduction of over-fitting.\n",
    "\n",
    "## Bagging or Bootstrap Aggregating\n",
    "Bagging trains each model in the ensemble using a randomly drawn subset of the training set. Then lets each model in the ensemble vote with equal weight. The random forest algorithm combines random decision trees with bagging to achieve high classification accuracy while preserving model generality, which is one of the reasons why the Random Forest classifier is very popular.\n",
    "\n",
    "## Curse of Dimensionality\n",
    "In machine learning one of the biggest difficulty is dealing with high number of dataset features or lack of enough data points to cover them. As an example, when a naive model requires $5$ data points on each feature of a $100$ feature dataset, the **joint** density function would ideally necessitate $5^{100}$ data points, so that the density function could be computed perfectly. This is not only too big of a dataset size but also it might be impossible to acquire such a dataset. In practice, the curse of dimensionality is avoided by assuming the features are independent, such as the Naive Bayes assumption. The alternative approach is using ensembles of weak classifiers and bagging, so that each classifier would work on a subset of features.\n",
    "\n",
    "## Boosting\n",
    "Boosting trains several weak learning algorithms and combine (i.e. summation) their weighted results. Boosting builds an ensemble by training each new model instance in such a way to improve upon the previous models mis-classify. The most common implementation of boosting is Adaboost where the classifier is composed of $T$ many classifiers such that $F_T(x) = \\sum_{t=1}^{T} f_t(x)$. The training is done such that the error is minimized at every iteration, $E_T = \\sum_i E (F_{t-1}(x_i) + \\alpha_t h(x_i)$\n",
    "\n",
    "## Random Forest\n",
    "he Random Forest classifier is a method that combines the decision trees and ensemble learning. The forest is composed of many trees that use randomly picked data features (attributes) as their input. The forest generation process constructs a collection of trees with controlled variance. The resulting prediction can be decided by majority voting or weighted voting. Random Forests have several advantages, such as a low number of control and model parameters, resistance to overfitting, no requirement for feature selection or feature reduction because they can use a large number of potential attributes. If some features are not useful for prediction, the algorithm will simply ignore them. One important advantage of Random Forest is that the variance of the model decreases as the number of trees in the forest increase, whereas the bias remains the same.\n",
    "\n",
    "Random Forests have some disadvantages such as low model interpretability, performance loss due to correlated (dependent) variables, and dependence on the random number generator of the implementation. Note that all these disadvantages (except for the third one which is relatively easy to fix) are inherent to several other popular classifiers.\n",
    "\n",
    "### Implementation Details\n",
    "Number of features in each tree can be set to either $\\sqrt{M}$, or $\\log_2(M+1)$ for a dataset $X \\in \\mathbb{R}^{N \\times M}$\n",
    "\n",
    "Ensemble size determination remains to be a challenge. In practice a size is picked subjectively according to the number of features and data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
