{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T12:23:44.989064Z",
     "start_time": "2020-08-06T12:23:44.986800Z"
    }
   },
   "source": [
    "# Data Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset of  ð‘€  variables and  ð‘  dataset points, a feature is one of the **independent** variables in a dataset and also called as a **predictor**. Generally the input to a machine learning program is a column of a tabular dataset where each row (of  ð‘  rows) is a dataset point in  ð‘€  dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Jupyter notebooks we will use the matrix  ð‘‹ð‘Ã—ð‘€  as the dataset symbol without the dependent variable (also called the label, category, class, predicted variable) and we will store the dependent variable in the vector  ð‘¦ð‘Ã—1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ð‘‹  is a matrix. It's rows are data points and it's columns are the features. All classifiers in scikit-learn can understand this data format which is based on numpy 2-dimensional arrays. Thus for each input data point we have  ð‘¥âˆˆâ„ð‘€ , and we have  ð‘  data points to be used in our ML pipelines. We also have  ð‘¦âˆˆâ„¤  in general as the category. As an example, if we have  ð¾  categories, then  ð‘¦âˆˆ{ð‘˜:0â‰¤ð‘˜â‰¤ð¾,ð‘˜âˆˆâ„¤}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** In the following  ð‘‹  matrix we have 3 features and 5 data points, i.e.  ð‘€=3  and  ð‘=5 . We also have  5  values for dependent variable  ð‘¦ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© Guven\n",
    "Data Features \n",
    "Introduction\n",
    "Given a dataset of  ð‘€  variables and  ð‘  dataset points, a feature is one of the independent variables in a dataset and also called as a predictor. Generally the input to a machine learning program is a column of a tabular dataset where each row (of  ð‘  rows) is a dataset point in  ð‘€  dimensional space.\n",
    "\n",
    "In our Jupyter notebooks we will use the matrix  ð‘‹ð‘Ã—ð‘€  as the dataset symbol without the dependent variable (also called the label, category, class, predicted variable) and we will store the dependent variable in the vector  ð‘¦ð‘Ã—1 .\n",
    "\n",
    "ð‘‹  is a matrix. It's rows are data points and it's columns are the features. All classifiers in scikit-learn can understand this data format which is based on numpy 2-dimensional arrays. Thus for each input data point we have  ð‘¥âˆˆâ„ð‘€ , and we have  ð‘  data points to be used in our ML pipelines. We also have  ð‘¦âˆˆâ„¤  in general as the category. As an example, if we have  ð¾  categories, then  ð‘¦âˆˆ{ð‘˜:0â‰¤ð‘˜â‰¤ð¾,ð‘˜âˆˆâ„¤} \n",
    "Example: In the following  ð‘‹  matrix we have 3 features and 5 data points, i.e.  ð‘€=3  and  ð‘=5 . We also have  5  values for dependent variable  ð‘¦ .\n",
    "\n",
    "ð‘‹=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢ð‘¥11ð‘¥21ð‘¥31ð‘¥41ð‘¥51ð‘¥12ð‘¥22ð‘¥32ð‘¥42ð‘¥52ð‘¥13ð‘¥23ð‘¥33ð‘¥43ð‘¥53âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥ ,  ð‘¦=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢ð‘¦1ð‘¦2ð‘¦3ð‘¦4ð‘¦5âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥  â€ƒâ€ƒâ€ƒâ€ƒâ€ƒ Such that in practice,  ð‘‹=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢4.94.74.65.5.43.3.23.13.63.91.41.31.51.41.7âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥ ,  ð‘¦=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢10012âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the dependent variable  ð‘¦  has levels from the alphabet  Î£={0,1,2} , i.e. there are 3 categories in the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted variable or label is the **dependent** variable and it is dependent on the **independent** variables or features. This amount of dependence can be sometimes high and sometimes very low depending on the dataset or the nature of the problem (again, dataset expresses this). If there is no correlation (or fully independent), then the dataset may not be suitable for the problem at hand and/or we may have to remove that feature from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, for numerical variables, the Pearson correlation coefficient of two variables  ð‘¥  (lower case x, a feature) and  ð‘¦  is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ð‘Ÿð‘¥ð‘¦=âˆ‘ð‘ð‘–=1(ð‘¥ð‘–âˆ’ð‘¥Â¯)(ð‘¦ð‘–âˆ’ð‘¦Â¯)âˆ‘ð‘ð‘–=1(ð‘¥ð‘–âˆ’ð‘¥Â¯)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšâˆ‘ð‘ð‘–=1(ð‘¦ð‘–âˆ’ð‘¦Â¯)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš , where  ð‘¥Â¯  and  ð‘¦Â¯  are sample means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we need a good correlation (close to 1 or -1) between the independent and dependent (predicted) variables so our ML model would actually work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can correlation coefficient be used for determining important features for the machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numerical - Can be integer  â„¤  or floating point  â„ . Generally it is safe to convert all numerical variables to floating point variables.\n",
    "* Nominal - The variable values are drawn from a finite set of levels or from an alphabet  Î£ .\n",
    "* Binary - The variable values can be either 0 or 1 (or, False or True). Some algorithms work fast on this kind of values, especially constrained optimization related methods.\n",
    "* String - May not be used directly unless the ML program (preprocessing) knows how to deal with it\n",
    "* Date - May not be used directly unless the ML program (preprocessing) knows how to deal with it. It might be a good idea to convert (or map) dates to some integer numbers - for example, Excel handles dates in this manner.\n",
    "* More complicated features, e.g. a DNA sequence (sequence of {A,C,G,T} letters) - Other, simpler, features need to be extracted from the input sequence so that this higher-level feature can be used in an ML program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal to Numerical Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible way of converting nominal variables to numerical is one-hot encoding:\n",
    "\n",
    "1. During preprocessing count the number of levels in the set of possible levels a nominal variable  ð‘£ð‘›ð‘œð‘š  takes. Such as,  ð¿  different levels,  ð‘˜=1,...,ð¿ .\n",
    "2. Create  ð¿  binary variables for that nominal variable  ð‘£ð‘›ð‘œð‘š  where each row will have a binary zero for  ð¿âˆ’1  binary variables except for the jth level which corresponds to the level-j when  ð‘£ð‘›ð‘œð‘š  takes a value of level-j.\n",
    "\n",
    "Following above procedure, a nominal variable with a cardinality of  ð¿  results in  ð¿  many binary variable creation (and dropping the original nominal variable itself). In other words, each unique level of that nominal variable is mapped to a binary variable. Note that, for the sake of this representation, storage space is wasted.\n",
    "\n",
    "Also observe that the one-hot encoded variables are like unit vectors of linear algebra.\n",
    "\n",
    "Conversion of nominal variables to numerical is an important step for many numerical-only classifiers, such as neural networks, support vector machines, and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nominal variable  ð‘‡  take levels from the  Î£={low,medium,high} . Numerical conversion involves each unique level being mapped to one of the  ð‘‡ð‘–  binary vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:09:10.888267Z",
     "start_time": "2020-08-06T13:09:10.884848Z"
    }
   },
   "source": [
    "Nominal Variable  ð‘‡ \t ð‘‡0 \t ð‘‡1 \t ð‘‡2 \n",
    "low\t1\t0\t0\n",
    "medium\t0\t1\t0\n",
    "low\t1\t0\t0\n",
    "high\t0\t0\t1\n",
    "high\t0\t0\t1\n",
    "low\t1\t0\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical to Nominal Conversion\n",
    "Generally, histograms, binning and bin boundaries are used to group numerical values into levels or one-hot encoded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Dataset Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI KDD online repository has various datasets which can be used for analysis, machine learning and several application fields, such as GIS, cybersecurity, NLP, etc. The origin of some datasets go back to more than 20 years sourced from competitions, challenges, grants, etc. Researchers and students use these datasets and share their experiences using a common platform.\n",
    "\n",
    "Source: UCI Knowledge Discovery in Databases Archive http://kdd.ics.uci.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle** data repository has various datasets which are used for Kaggle competitions. The web site also has tools to examine the features on-site. This source is one of the largest.\n",
    "Go to the Kaggle dataset source: https://www.kaggle.com/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDnuggets** is another web page which encompasses almost everything (posts, news, datasets, tutorials, forums, webinars, software, etc.) that is relevant to machine learning and data analysis.\n",
    "Source: KDnuggets Datasets for Data Mining and https://www.kdnuggets.com/datasets/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook will demonstrate three different datasets from these repositories.\n",
    "\n",
    "* UCI KDD archive  â†’  1990 US Census data\n",
    "* Kaggle  â†’  Graduate Admissions data\n",
    "* Kaggle  â†’  The Human Freedom Index data\n",
    "\n",
    "Download the data files from UCI KDD web site and Kaggle web site (by registering to Kaggle -using a disposable email address- if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** About physical dataset file shared among teams. Comparing machine learning models, and measuring performances for model selection is heavily dependent on the input dataset. Thus, if a comparison between models and a comparison among different experiments or teams results are at hand, then the dataset shared among teams or different set of experiments must be exactly the same dataset. Moreover, to ensure the validity, the exact same file should be shared among multiple teams or between different models pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we use the downloaded and previously cleaned data files:\n",
    "\n",
    "* USCensus1990.data.csv\n",
    "* Admission_Predict.csv\n",
    "* hfi_cc_2018_cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:12:51.711560Z",
     "start_time": "2020-08-06T13:12:51.707072Z"
    }
   },
   "source": [
    "Note that there are two dataset cleaning tasks before a machine learning model development can begin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:14:20.492896Z",
     "start_time": "2020-08-06T13:14:20.487561Z"
    }
   },
   "source": [
    "* Cleaning the data so the framework understands the data right, i.e. formatting, removing confusing symbols, quotes, etc.\n",
    "* Cleaning (preprocessing) the data to improve the ML task, i.e. imputing values, removing outliers, removing incorrect dataset values, deriving variables, selecting variables, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both cleaning steps are crucial in preparing the dataset for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quote:** \"As data scientists, our job is to extract signal from noise.\" (ref: KDnuggets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:16:09.587166Z",
     "start_time": "2020-08-06T13:16:09.585253Z"
    }
   },
   "source": [
    "Let's see what our data files contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:00.503679Z",
     "start_time": "2020-08-06T14:38:00.061129Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('../datasets/USCensus1990.data.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:00.505602Z",
     "start_time": "2020-08-06T14:38:00.063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Locate and load the data file\n",
    "df = pd.read_csv('../datasets/Admission_Predict_Ver1.1.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:00.506838Z",
     "start_time": "2020-08-06T14:38:00.064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Locate and load the data file\n",
    "df = pd.read_csv('../datasets/hfi_cc_2018.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Human Freedom Index DatasetÂ¶\n",
    "Opening the hfi_cc_2018 CSV data file in Weka is tricky as it needs two modifications as in below:\n",
    "\n",
    "* Using a text editor, change the value \"d'Ivoire\" to \"dIvoire\" by removing the single quote. The single quote is used by Weka to mark nominal variables.\n",
    "* Using a text editor add single quotes to the feature name region to mark it as nominal. Weka wants to see single quotes in the variable name (in the header) to be able to load the type of the variables correctly.\n",
    "\n",
    "\n",
    "This particular example shows that data mining, machine learning frameworks such as Weka have their own standards that the model developer has to pay attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weka Framework\n",
    "\n",
    "Weka is a data analytics framework (open-source, Java based) with very strong ML and data mining abilities. To install:\n",
    "\n",
    "1. Download and install 64-bit Java JRE https://www.java.com/en/download/\n",
    "2. Download Weka Linux distro zip file from https://www.cs.waikato.ac.nz/ml/weka/downloading.html\n",
    "\n",
    "and extract the zip to C:\\weka on your computer's local disk. Use Windows command prompt:\n",
    "\n",
    "1. Check the version of Java: java -version so that make sure the java on the path is reflected to the one downloaded.\n",
    "2. On a command prompt, run java -Xmx8g -jar c:\\weka\\weka.jar (8GB heap space to be used for big data files - make sure your computer supports, or adjust this value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WATCH THE RELATED LECTURE for opening, using Weka, preprocessing and running Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate Admissions Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the data file Admission_Predict.csv in Weka. Click on Explorer and open the CSV file with Open File button. We need a dependent variable to predict or do something with it. Let's pick A9 - Chance of Admit (A_XX is the attribute which starts from index 1). We need to convert this variable to a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:21:01.031056Z",
     "start_time": "2020-08-06T13:21:01.026868Z"
    }
   },
   "source": [
    "1. In Filter, AddExpression -E \"ifelse (A9 > 0.9, 1, 0)\" -N Admit then press Apply\n",
    "2. In Filter, NumericToNominal -R last\n",
    "3. In Filter, RenameNominalValues -R last -N \"0:No, 1:Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing pick the RandomForest (RF) classifier from Classifier-Trees-RandomForest. Run it with 10-fold cross validation, with Start button. Observe the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why do you think RF model performance results 100%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove the variable \"Serial No.\" (Why useless?) and remove \"Chance of Admit\" variable (Remove button down below). Remember we categorized it to the variable named \"Admit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:21:58.069956Z",
     "start_time": "2020-08-06T13:21:58.066732Z"
    }
   },
   "source": [
    "**Question:** Why do you think RF model performance is less than 100% now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Below cell can be safely ignored. This is the code to display markdown tables left oriented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:00.507992Z",
     "start_time": "2020-08-06T14:38:00.068Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
