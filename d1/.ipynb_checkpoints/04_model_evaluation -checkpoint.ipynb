{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:06:33.582905Z",
     "start_time": "2020-08-06T13:06:33.579958Z"
    }
   },
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the machine learning model and inherently the development pipeline is an essential part of any **learning** project. There are two perspectives:\n",
    "\n",
    "* Verifying and validating the learning approach/pipeline/model\n",
    "* Deploying the ML model\n",
    "\n",
    "Verifying the model always involves a training and testing dataset and frequently a validation dataset.\n",
    "\n",
    "1. The training, testing, and validation datasets are drawn from the same main dataset. Thus any underlying probabilistic distribution of the collected and preprocessed dataset is assumed to apply to all these three pieces.\n",
    "2. Training dataset is used to develop the model. Training dataset is generally non-overlapping, or in other words, samples are drawn without replacement.\n",
    "3. Validation dataset is used to fine-tune the model. Grid search model parameters and pick the best performance based on the validation performance only. Then the testing dataset is used to actually measure the trained and tuned model. If a model does not need a validation dataset (i.e. no parameters to tune, such as Naive Bayes), then we can add the validation dataset to the training dataset.\n",
    "4. Testing dataset is used to verify and evaluate the model performance.\n",
    "\n",
    "**Important:** Though evaluation is about measuring the performance, it is also setting the details of the model development pipeline in such a way that the exact pipeline will also be used for the **product** development or deployment.\n",
    "\n",
    "Note that through model evaluation we are making sure that what we have done as model development is correct. If there is a next project step where we **deploy** the model in a computer system (in fact it would be our actual product), so that our developed model would actually predict something, then the most common approach is using the entire dataset to train and develop the model, exactly as the evaluation pipeline is constructed. Then a possible overall approach would be use train/test/validate to build and verify the pipeline, and compute some metrics so that we can scholarly show that the developed model actually learned something and performs better than a **random** classifier.\n",
    "\n",
    "**Question:** If there are 5 target classes, then what is the lower-bound accuracy a random classifier would achieve?\n",
    "\n",
    "Model evaluation answers a few questions:\n",
    "\n",
    "* Classification - What is the expected accuracy a model would achieve when given unseen data points and asked to predict?\n",
    "* Classification - What is the expected accuracy of individual classes and confusion between them (i.e. when the class A is truth for a particular data point, is class B is predicted, or class C is predicted?)\n",
    "* Regression - What is the total error between the regression curve (simplified model, fitted curve, etc.) and the actual data points?\n",
    "* If we are OK with a particular False-Alarm-Rate, then what is the highest accuracy we can achieve? For example, based on Receiver Operating Characteristic (ROC) curve what should be our operating point.\n",
    "* What is the variation of the classification error?\n",
    "* Can we estimate the model generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:21:35.009066Z",
     "start_time": "2020-08-06T14:21:35.007162Z"
    }
   },
   "source": [
    "## Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training-testing-validation:** Common approach. In our project, we have to come up with a suitable training-testing-validation dataset. A good example can be as in the following: Given 20 year long dataset where we know the data is collected every day for 20 years, the training dataset can use all those 20 years. The validation dataset can use all 20 years but a percentage portion of the training (e.g. 10%). The testing dataset can be drawn from the last 5 years. Assuming the unseen data would be somewhat closer to the recent years. This kind of decisions are to be made by the model developer working with the subject matter experts.\n",
    "\n",
    "**K-fold cross validation:** Common approach. Parameter tuning is not done but parameters are fixed throughout the entire development and evaluation. Each fold has non-overlapping test and train datasets. Average the computed accuracies for each fold and report as the accuracy. Clearly, a ML method with **good generalization** and rather **insensitive to model parameters** would work much better with this evaluation model.\n",
    "\n",
    "**Leave-one-out-cross-validation (LOOCV):** Seldom used. Similar to k-fold cross validation but testing dataset is always a single point. Recommended for very small datasets or when several outlier data points exist in the set and they are to be learned by the model.\n",
    "\n",
    "**Stratification:** When target categories are unbalanced (e.g. there are more benign cases rather than malignant), then a correct validation requires the training/test datasets, or folds, preserving the percentage of samples for each class. sklearn library supports stratification fully, such as by the use of StratifiedKFold library function.\n",
    "\n",
    "The **most important aspect** of model evaluation is measuring the **expected** real-world performance with a good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model's ability to adapt (not a real adaptation but more like being ready) properly to new, previously unseen data, drawn from the same probabilistic distribution as the one used to create the model. Some classifiers which have good generalization ability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Support Vector Machines (large-margin) classifiers, since they solve an optimization problem to maximize the distance between data points and the decision (discrimination) surface.\n",
    "* Random Forest (ensemble) classifiers, since they use few features per classifier, but numerous simple classifiers which randomly employ features, and then take the majority of the predictions made by this ensemble of classifiers.\n",
    "* Principal Component Analysis (PCA) for unsupervised learning, since they try to find simplified data representation by looking at the largest eigenvalues of the covariance matrix of features.\n",
    "* Human beings 😃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:28:15.725356Z",
     "start_time": "2020-08-06T14:28:15.721553Z"
    }
   },
   "source": [
    "Following is a list of evaluation metrics.\n",
    "\n",
    "1. Classification Accuracy\n",
    "This the major metric we will use.\n",
    "\n",
    "Accuracy =  𝚗𝚞𝚖𝚋𝚎𝚛 𝚘𝚏 𝚌𝚘𝚛𝚛𝚎𝚌𝚝 𝚙𝚛𝚎𝚍𝚒𝚌𝚝𝚒𝚘𝚗𝚜𝚝𝚘𝚝𝚊𝚕 𝚗𝚞𝚖𝚋𝚎𝚛 𝚘𝚏 𝚙𝚛𝚎𝚍𝚒𝚌𝚝𝚒𝚘𝚗𝚜\n",
    "\n",
    "2. Confusion Matrix¶\n",
    "This is the second major metric and it is defined for binary classification.\n",
    "\n",
    "N=33\ttruth A\ttruth B\n",
    "predicted A\t10\t3\t\n",
    "predicted B\t1\t20\t\n",
    "Historically from statistics, a positive and a negative class is defined, such as patient has cancer is positive, and does not have is negative. Assume class A is positive and class B is negative.\n",
    "\n",
    "Type I error - missing the positive, mis-predict class A - patient has cancer and missed.\n",
    "Type II error - patient does not have cancer but misdiagnosed - False Alarm.\n",
    "\n",
    "Type I errors are very costly.\n",
    "\n",
    "True Positive (TP) = Predicted A, truth is A\n",
    "True Negative (TN) = Predicted B, truth is B\n",
    "False Positive (FP) = Predicted A, truth is B - Type II error\n",
    "False Negative (FN) = Predicted B, truth is A - Type I error - major mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** TP=10, TN=20, FP=3, FN=1\n",
    "\n",
    "Accuracy =  𝚃𝙿+𝚃𝙽𝚃𝙿+𝚃𝙽+𝙵𝙿+𝙵𝙽=10+2010+20+3+1 \n",
    "TP Rate (TPR) =  𝚃𝙿∑𝚝𝚛𝚞𝚝𝚑 𝚙𝚘𝚜𝚒𝚝𝚒𝚟𝚎=1010+1 \n",
    "TN Rate (TNR) =  𝚃𝙽∑𝚝𝚛𝚞𝚝𝚑 𝚗𝚎𝚐𝚊𝚝𝚒𝚟𝚎=2020+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also from Information Retrieval, following metrics are defined,\n",
    "\n",
    "Precision =  𝚃𝙿𝚃𝙿+𝙵𝙿  How many selected items are relevant?\n",
    "\n",
    "Recall =  𝚃𝙿𝚃𝙿+𝙵𝙽  How many relevant items are selected?\n",
    "\n",
    "3. F1-score\n",
    "Also known as harmonic mean of precision and recall.\n",
    "\n",
    "F-1 score =  2Precision×RecallPrecision+Recall \n",
    "F score attempts to find a balance between precision and recall\n",
    "\n",
    "4. Area Under Curve (AUC)\n",
    "Also known as area under Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    "It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "5. Mean Absolute Error\n",
    "(MAE) is the average error between the original  𝑥𝑖  and the predicted  𝑥̂ 𝑖  values for regression problems.\n",
    "\n",
    "MAE =  1𝑁∑𝑖=1𝑁|𝑥𝑖−𝑥̂ 𝑖|\n",
    "\n",
    "6. Mean Squared Error (MSE)\n",
    "MSE is similar to MAE and is more common than MAE.\n",
    "\n",
    "MSE =  1𝑁∑𝑖=1𝑁(𝑥𝑖−𝑥̂ 𝑖)2 \n",
    "7. Logarithmic Loss\n",
    "Also known as Log Loss measures false classifications and suitable for multi-class classification (rather less common error metric).\n",
    "\n",
    "LogLoss =  −1𝑁∑𝑖=1𝑁∑𝑗=1𝑀𝑦𝑖𝑗log(𝑝𝑖𝑗) \n",
    "𝑦𝑖𝑗  indicates whether sample  𝑖  belongs to class  𝑗  or not\n",
    "𝑝𝑖𝑗  indicates the probability (or score) of sample  𝑖  belonging to class  𝑗 \n",
    "LogLoss has no upper bound and it exists on the range  [0,∞) . Log Loss measure close to 0 indicates a higher accuracy. In general, minimizing LogLoss gives greater accuracy for the classifier.\n",
    "\n",
    "## Receiver Operating Characteristic Curve\n",
    "The ROC curve is composed of true positive rate (TPR) on the y-axis and the false positive rate (FPR) on the x-axis while each operating point corresponds to some detection threshold or a classifier model parameter.\n",
    "\n",
    "The best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing zero false negatives and zero false positives, and perfect prediction. A random guess or a coin flip would result a point along the diagonal line (also called line of no-discrimination) from the left bottom to the top right corners (regardless of the positive and negative base rates).\n",
    "\n",
    "ROC can be used for both model evaluation (such as the area under the ROC curve, presents the model behavior with different parameters) and, or deciding on a model parameter for deployment, i.e. setting the TPR with a given accepted false alarm rate. ROC is used to make a model selection or set an operating classifier threshold by considering the TPR-FPR together. Generally, ROC is used for binary classification.\n",
    "\n",
    "An example ROC curve is generated in cells below using breast cancer data from sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:34.844312Z",
     "start_time": "2020-08-06T14:37:33.933708Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Locate and load the data file\n",
    "bc = load_breast_cancer()\n",
    "bc_df = pd.DataFrame(data= np.c_[bc.data, [bc.target_names[v] for v in bc.target]],\n",
    "                     columns= list(bc.feature_names)+['cancer'])\n",
    "# See how the data looks like\n",
    "bc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:34.850550Z",
     "start_time": "2020-08-06T14:37:34.846576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Populate the dataset, cancer column is target variable\n",
    "X = bc_df.loc[:, bc_df.columns != 'cancer'].values\n",
    "y = bc_df.loc[:, bc_df.columns == 'cancer'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:34.976921Z",
     "start_time": "2020-08-06T14:37:34.852411Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Display OP\n",
    "def annot(opi, x, y):\n",
    "    plt.annotate(f\"OP{opi}\", xy=(x, y), xytext=(.90*x+.1, .80*y), arrowprops=dict(facecolor='lightgray', shrink=1))\n",
    "\n",
    "# Training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.98, random_state=14)\n",
    "\n",
    "# Parameter to vary for Logistic Regression\n",
    "C = (2e-1, 0.5, 0.8, 1, 2, 5, 1e1, 2e1, 1e2)\n",
    "\n",
    "# Let's vary C and generate training/testing sessions to collect data for ROC\n",
    "FPR, TPR = [], []\n",
    "for c in C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14,\n",
    "                                               penalty='l1',\n",
    "                                               solver='liblinear',\n",
    "                                               class_weight='balanced',\n",
    "                                               C=c,\n",
    "                                               multi_class='auto',\n",
    "                                               max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    y_pred = pipe_lr.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]  # Pd\n",
    "    FPR += [fp/(fp+tn)]  # Pf\n",
    "    #\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:35.225704Z",
     "start_time": "2020-08-06T14:37:34.979183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sorts the points to display nicely on ROC\n",
    "FPR, TPR = zip(*sorted(zip(FPR, TPR)))\n",
    "fpr = [0.]+list(FPR)+[1.]; tpr = [0.]+list(TPR)+[1.]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(fpr, tpr, ':', label='ROC')\n",
    "plt.scatter(FPR, TPR, 50, color='red', marker='o', label='operating points')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color=(0.6, 0.6, 0.6), label='coin flip')\n",
    "\n",
    "# Annotate certain operating points\n",
    "annot(1, fpr[1], tpr[1])\n",
    "annot(2, fpr[4], tpr[4])\n",
    "annot(3, fpr[8], tpr[8])\n",
    "annot(4, fpr[9], tpr[9])\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dramatization:** Note that in order to generate a nice ROC for demonstration purposes, we selected a harsh test size of 98% and random_state=14. Also picked LogisticRegression with parameters penalty='l1', solver='liblinear'. Not every classifier generates an ROC curve like above due to non-existence of a parameter that can vary detection versus false alarm smoothly.\n",
    "\n",
    "**Important Question:** Given above ROC, which operating point would you pick for cancer detection? OP1, OP2, OP3, or OP4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error\n",
    "From statistical learning theory, the generalization error is the difference between the expected and empirical error. Or, the difference between error on the training set and error on the underlying joint probability distribution.\n",
    "\n",
    "Question: If we know the underlying joint probability distribution of the data, then would we need an ML method?\n",
    "\n",
    "Answer: No. We will probably never be able to know the underlying data probability distribution for practical ML problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Student: Below cell can be safely ignored. This is the code to display tables left oriented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:35.230704Z",
     "start_time": "2020-08-06T14:37:35.227211Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
