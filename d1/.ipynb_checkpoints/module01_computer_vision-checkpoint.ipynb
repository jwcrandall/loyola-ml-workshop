{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Machine Learning in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is the art of solving a computation problem using a computer without an explicit program. ML is so pervasive today that various ML applications such as image recognition, stock trading, email spam detection, product recommendation, medical diagnosis, predictive maintenance, cybersecurity, etc. are constantly used by organizations around us and probably sometimes without our awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognizer ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [scikit-learn.org examples]\n",
    "\n",
    "[scikit-learn.org examples]: https://scikit-learn.org/stable/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing hand-written digit images so that they can be converted to numbers (in the format a computing system could use) and be easily processed by automated systems, e.g. hand written zip codes on letters (snail mail) for sorting letters according to zip codes for faster and cheaper processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T11:41:11.895000Z",
     "start_time": "2020-08-06T11:41:11.890956Z"
    }
   },
   "source": [
    "Possibly there will be a camera capturing images of envelope faces one by one, and a computer program to extract sections of digit images from these camera images to be used in the computing pipeline.\n",
    "Our job as machine learning model developer is to pre-process the captured dataset (i.e. segmented pictures of hand written digit images), apply ML algorithms, and evaluate and verify the performance of the developed model meets the performance goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised-learning** zip code digit images from a proper dataset.\n",
    "1. Take digital snapshots of letter faces\n",
    "2. Locate zip codes in images\n",
    "3. Sement images to find each digit\n",
    "4. Convert (i.e. scale, stretch, etc.) each digit image to 8x8 images as feature vectors\n",
    "5. Label the training image set by subject matter human experts for ground truth\n",
    "6. Pass the labeled training image dataset to a classifier to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps (1.) to (5.) above are data preparation stage. Once the model is generated (i.e. trained), feature vectors extracted from **never-seen-before** data can be **tested** to **predict** these new digit images. This new data is called testing dataset.\n",
    "\n",
    "The labels used by human experts to mark the training dataset would be the **category**, or class, or group that a digit image would belong to. The category is also similar to a **feature** given by the dataset for classification (in this example it is not obvious). To make a distinction between the data set feature (a column in the training dataset matrix) and the group feature we want to find out for a new data point, we call it a **category** or a class. Here the prediction is finding out (with or without a score or a probability) which category the new data point would belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import open-source libraries \n",
    "Open source libraries such as NumpPy, SciPy, matplotlib, scikit-learn are developed and maintained by tens of thousands of developers, researchers, engineers from various organizations like universities and industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "[numpy] [scikit-learn]\n",
    "\n",
    "[scikit-learn]: https://scikit-learn.org/stable/modules/classes.html\n",
    "[numpy]: https://docs.scipy.org/doc/numpy/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we use Python in this course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Python is taking over as the number 1 data science programming language ([reference])\n",
    "\n",
    "[reference]: https://opensource.com/article/18/5/numbers-python-community-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:05.114750Z",
     "start_time": "2020-08-06T14:38:04.417785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library sklearn (short proxy for scikit-learn) includes several datasets to give the opportunity to users for a data science quick-start. In the following notebook cells,the digit data set is used to train a classifier model. These digit images are from MNIST dataset collected 20 years ago from actual hand-written letters to automate USPS zip code sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:05.180389Z",
     "start_time": "2020-08-06T14:38:05.116841Z"
    }
   },
   "outputs": [],
   "source": [
    "# The digits dataset for training\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data is made of 8x8 images of digits\n",
    "# zip the image and label (dependent variable) together\n",
    "images_and_labels = list(zip(digits.images, digits.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the problem dataset to gain insight. This step is crucial as the model developer (i.e. data scientist) will have better understanding of the data set to solve the problem. Many questions will be answered towards using the right features, doing the right **feature engineering**, picking the right learning model, and avoiding bugs, etc. The representation of the dataset might not conform to what the developer might expect. Example, what is the computer representation of the data set? A matrix? Which data structure? A row of a matrix is a data point? Are ground truth labels in this matrix? Do we have missing data? What about outliers? Which features are **numerical** and which are **nominal** and which are neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the number format of the data? Integer? Floating point? Long? Double?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:05.185335Z",
     "start_time": "2020-08-06T14:38:05.182551Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of images in the training set, N= {len(images_and_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.333052Z",
     "start_time": "2020-08-06T14:38:05.187254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the first 40 data points - in this case images\n",
    "plt.figure(1, figsize=(20, 10))\n",
    "for index, (image, label) in enumerate(images_and_labels[:40]):\n",
    "    plt.subplot(4, 10, index+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f'Training: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.340568Z",
     "start_time": "2020-08-06T14:38:06.335720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how the data looks like, examine the label as the last element\n",
    "print(images_and_labels[0])\n",
    "print(images_and_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.345501Z",
     "start_time": "2020-08-06T14:38:06.342101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the size of the data vectors, examine the indices\n",
    "print(len(images_and_labels))\n",
    "print(len(images_and_labels[0]))\n",
    "print(len(images_and_labels[0][0]))\n",
    "print(len(images_and_labels[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Above numbers make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the times the data pre-processing stage takes bulk of the data science task.\n",
    "Pre-processing includes correcting number formats, setting ranges, converting feature types, checking for erroneous input (e.g. February 31, 2119), erroneous data entry (e.g. 5 year old child with a Ph.D. degree), cleaning outliers that can throw off the model, **imputing** missing values for robust model generation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a reasonable, good, sufficient (all of these properties are clearly subjective) data set is achieved, then the learning stage is a short script or pressing a few buttons in a program in the data science platform we use, such as Weka (source: https://www.cs.waikato.ac.nz/~ml/weka/index.html) or a Python pipeline using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data reduction, feature engineering or feature selection process is within or overlapping with the pre-processing stage. Reducing data helps generation of robust models with high **generalization** ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pitfall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any error made in the input dataset will be reflected on the generated model and test set outputs (i.e. predictions). In data science, it is very easy to trick ourselves about a model performance by committing data errors during preparation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.350522Z",
     "start_time": "2020-08-06T14:38:06.347141Z"
    }
   },
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image\n",
    "# Turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "print(f'N={len(data)}, M={len(data[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) were very popular machine learning methods (algorithms) which gained popularity a lot over Artificial Neural Networks (ANN) before the deep learning time has begun. There are a few advantages and disadvantages between them:\n",
    "\n",
    "* Early ANN methods were mostly heuristic, trial-and-error approaches, where SVM has a sound theory\n",
    "* ANN minimizes empirical risk (or cost), SVM minimizes structural risk (maximizes the margin between the data points and the discriminating hyperplane)\n",
    "* If done right, both ANN and SVM would generate identical decision surfaces after successful training stages\n",
    "* SVM solves the problem by finding the optimum solution (solving a real convex optimization problem â€“ because the algorithm sets up the problem accordingly) while ANN looks for a solution with gradient descent optimization algorithm. Literally, ANN fits a hypersurface (decision surface) to the data and that is why ANN are also called function approximators.\n",
    "* Local minima is problem for ANN classifiers, since the optimization is greedy\n",
    "* Setting the network structure is a problem of ANN model development\n",
    "* Selection of a suitable kernel and its parameters is a problem of SVM model development\n",
    "* SVM has very high generalization ability and automatically sets the model size (with support vectors)\n",
    "\n",
    "In the following cell SVM is picked as the classifier for the digit recognition problem. Prediction stage will generate the class labels the model predicts for the new (never-seen-before) test data input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.438422Z",
     "start_time": "2020-08-06T14:38:06.353774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "# gamma is normally determined using a hyperparameter search which would need a validation dataset\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first half of the digits - 50% data is used as the training set\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above model is stored in the `classifier` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classification model (i.e. trained model generated by the learning SVM program) an evaluation has to be conducted to measure the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural metric: The ratio of the number of data points the model predicts correct over the total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AğšŒğšŒğšğš›ğšŠğšŒğš¢ = ğš—ğšğš–ğš‹ğšğš› ğš˜ğš ğšŒğš˜ğš›ğš›ğšğšŒğš ğš™ğš›ğšğšğš’ğšŒğšğš’ğš˜ğš—ğšœ / ğš—ğšğš–ğš‹ğšğš› ğš˜ğš ğšğš˜ğšğšŠğš• ğš™ğš›ğšğšğš’ğšŒğšğš’ğš˜ğš—ğšœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When many categories are involved as in the above example (there are 10 classes - digits from zero to nine), a **confusion matrix** is used to present the accuracy. Each column shows predicted class and each row shows actual. An ideal confusion matrix is all 100% in all diagonal elements, and zeros everywhere except for the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **unbalanced** machine learning problem has one class much larger than the other(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an unbalanced problem (such as patient cancer prediction in healthcare), then Accuracy is not the best metric but rather, F1-score and similar metrics must be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reclassification** is defined as using the entire training dataset as the testing dataset after the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not just train the model on the entire dataset, test on the entire dataset and report how many predicted correctly over the total number data point.\n",
    "However note that reclassification performance can be used as a metric to indicate model **overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can we report the reclassification performance as our model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A right way of training, testing and evaluating an ML model requires the following generic stages:\n",
    "\n",
    "* Divide the dataset in a non-overlapping fashion for training set, testing set and validation set\n",
    "* Use training dataset to develop, generate the model\n",
    "* Use validation dataset to tune model hyperparameters (e.g. Radial Basis Function gamma parameter of an SVM)\n",
    "* Use test dataset to measure the probable real-world performance of the model\n",
    "\n",
    "**Question:** How would one deploy the generated ML model? Such as the trained as in the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.444702Z",
     "start_time": "2020-08-06T14:38:06.440442Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Classification report for classifier {classifier}:\\n{metrics.accuracy_score(expected, predicted)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below shows the prediction versus actual (dataset labels) on matrix where the rows are actual and columns are predicted classes. Since we have 10 classes in the MNIST digit dataset (for each digit), the matrix is  10Ã—10 . As can be seen, very few testing data points are confused with other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.450353Z",
     "start_time": "2020-08-06T14:38:06.445932Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Confusion matrix:\\n{metrics.confusion_matrix(expected, predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:06.664243Z",
     "start_time": "2020-08-06T14:38:06.451542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the indices in images_and_predictions below to see more predictions\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[30:34]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f'Prediction: {prediction}\\n - Ground Truth: {expected[index+30]}')\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, model development pipeline and performance evaluation is completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
