{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:06:44.230893Z",
     "start_time": "2020-08-06T13:06:44.227856Z"
    }
   },
   "source": [
    "# Preprocessing Datasets for Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the data acquired for real world problems are often incomplete, noisy, and inconsistent. A few percentage of non-clean data points may effect the final performance by a few percentage drop. If a few steps of preprocessing were taken in the right direction, then better results would be easily achievable. A good data preprocessing is a necessary step for good machine learning performance and it is widely accepted that preprocessing takes the bulk of the overall machine learning effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to data \"cleaning\", certain algorithms require data features properties in certain ways, such as **normalized** and **standardized** to make the method work better. For example clustering approaches by distance measures require data features to be normalized. The following procedures are common steps in preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data formatting, cleaning\n",
    "* Discretization, one-hot encoding\n",
    "* Data integration and transformation\n",
    "* Data reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning frameworks, such as pandas, scikit-learn, Weka, expect dataset files to be in certain formats to be able to process them. The Comma Separated Values CSV is one of the most common file formats. When examining datasets sometimes we see the files might contain artifacts:\n",
    "\n",
    "* single quotes in double quotes, i.e. \"Cote d'Azor\" or reversed? e.g. 'Cote d'Azor'\n",
    "* single quotes to differentiate between strings and values. i.e. '1' or 1\n",
    "* use of semicolon instead of commas e.g. 1;50;red; in a row\n",
    "\n",
    "In addition to the data formats artifacts, we might also see:\n",
    "\n",
    "* duplicates of data lines (why is this undesired?)\n",
    "* missing values (marked as '?' in Weka or 'NaN' in pandas for numerical variables)\n",
    "* incorrect entries (e.g. clerical errors)\n",
    "\n",
    "Note that framework programs such as Weka learners are mature and strong enough to work with these problems without us cleaning them with a preprocessing stage. However, if we do the preprocessing ourselves, then we always increase the **quality of the dataset** and this helps the next stages of machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:31:18.993886Z",
     "start_time": "2020-08-06T13:31:18.989076Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the breast cancer dataset file located at the module page, named module03_breast_cancer.csv. Load it with pandas library and check for (1.) duplicates, (2.) missing values, (3.) incorrect entries. In the following cells, for each problem that the dataset has, a correction is provided once the situation is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:55.798295Z",
     "start_time": "2020-08-06T14:37:54.997340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries we always include\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('../datasets/module03_breast_cancer.csv')\n",
    "print(f'#rows={len(df)} #columns={len(df.columns)}')\n",
    "\n",
    "# Print some info and plots to have a feeling about the dataset\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:55.814626Z",
     "start_time": "2020-08-06T14:37:55.800472Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.349581Z",
     "start_time": "2020-08-06T14:37:55.816905Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bc(_df):  # Make sure use a '_variable' name to avoid shadowing variables in other cells\n",
    "    g = sns.FacetGrid(_df, col='deg-malig', hue='recurrence')\n",
    "    g.map(plt.scatter, 'age', 'tumor-size', alpha=.7)\n",
    "    g.add_legend()\n",
    "    plt.show()\n",
    "#\n",
    "plot_bc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:35:51.001017Z",
     "start_time": "2020-08-06T13:35:50.996784Z"
    }
   },
   "source": [
    "**Observe:** In the second plot what is that data point at age 250?? ...Hmmm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "Let's check duplicate values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.360068Z",
     "start_time": "2020-08-06T14:37:56.351803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates, this adds a new column to the dataset\n",
    "df[\"is_duplicate\"]= df.duplicated()\n",
    "\n",
    "# Note that when using f-strings, the internal quote character must be different, such as 'is_duplicate' above\n",
    "print(f\"#total= {len(df)}\")\n",
    "print(f\"#duplicated= {len(df[df['is_duplicate']==True])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.374889Z",
     "start_time": "2020-08-06T14:37:56.362236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print rows which have True in column 'is_duplicate'\n",
    "df[df['is_duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.382953Z",
     "start_time": "2020-08-06T14:37:56.376582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the duplicate rows using index - best way to drop in pandas\n",
    "index_to_drop = df[df['is_duplicate']==True].index\n",
    "df.drop(index_to_drop, inplace=True)\n",
    "\n",
    "# Remove the duplicate marker column\n",
    "df.drop(columns='is_duplicate', inplace=True)\n",
    "print(f'#total= {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:** Total number of rows (data points) reduced to 293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute missing values. If we do not handle missing values, then most of the times the ML algorithms will handle them internally.\n",
    "\n",
    "The safest and most common approach: Use mean (or equally acceptable median) for numerical values; and mode for nominal values to impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean:  ùë•¬Ø=1ùëÅ‚àëùëñ=1ùëÅùë•ùëñ \n",
    "Median:  ùë•ÃÉ =ùë•[|ùë•|/2]+ùë•[|ùë•|/2+1]2 \n",
    "Mode:  ùë•ÃÇ =ùöäùöõùöêùöñùöäùö°ùë•ùëì(ùë•)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.389029Z",
     "start_time": "2020-08-06T14:37:56.384450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do we have NaN in our dataset?\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.420904Z",
     "start_time": "2020-08-06T14:37:56.392242Z"
    }
   },
   "outputs": [],
   "source": [
    "# We do have NaN - three numerical variables - check first cell, it says float 64\n",
    "display(df[df['age'].isnull()])\n",
    "display(df[df['tumor-size'].isnull()])\n",
    "display(df[df['inv-nodes'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.440086Z",
     "start_time": "2020-08-06T14:37:56.423430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean values of columns\n",
    "print(f\"mean-age= {np.mean(df['age'])}\")\n",
    "print(f\"mean-tumor-size= {np.mean(df['tumor-size'])}\")\n",
    "print(f\"mean-inv-nodes= {np.mean(df['inv-nodes'])}\")\n",
    "\n",
    "# Impute\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "df['tumor-size'] = df['tumor-size'].fillna(df['tumor-size'].mean())\n",
    "df['inv-nodes'] = df['inv-nodes'].fillna(df['inv-nodes'].mean())\n",
    "\n",
    "# Check with the previous cell results\n",
    "display(df.loc[[24,25,26,27,28]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing nominal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding missing values in nominal variables is more tricky. First let's look at the nominal variables and then see what kind of unique values these nominal variables take. i.e. this is the level of the nominal variable drawn from a finite alphabet. Unless a numerical type (int64, float64, etc) df.dtype will correspond to an object which is a np.object class after read into from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.445565Z",
     "start_time": "2020-08-06T14:37:56.441671Z"
    }
   },
   "outputs": [],
   "source": [
    "# What are the column types?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.452188Z",
     "start_time": "2020-08-06T14:37:56.446876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check unique levels and see any marker is used for a missing level\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == np.object:\n",
    "        print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables node-caps and breast-quad has '?' levels which need to be imputed with values to help the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.460798Z",
     "start_time": "2020-08-06T14:37:56.453936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the next feature\n",
    "display(df['node-caps'].value_counts())\n",
    "print('mode-node-caps', df['node-caps'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.469386Z",
     "start_time": "2020-08-06T14:37:56.462569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the next feature\n",
    "display(df['breast-quad'].value_counts())\n",
    "print('mode-breast-quad', df['breast-quad'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.474702Z",
     "start_time": "2020-08-06T14:37:56.471016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace '?' with mode - value/level with highest frequency in the feature\n",
    "df['node-caps'] = df['node-caps'].replace({'?':'no'})\n",
    "df['breast-quad'] = df['breast-quad'].replace({'?':'left_low'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.481266Z",
     "start_time": "2020-08-06T14:37:56.476227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Again, check unique levels and see any marker is used or left out for a missing level\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == np.object:\n",
    "        print (col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the age value 250 from previous cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out incorrect entries is more difficult than the previous steps as incorrect entries truly depend on the data column and **domain knowledge.** For this step we will look at the plots of numerical columns and figure out possible incorrect entries. Also Subject Matter Experts (SME) would help greatly in real-world projects about incorrect entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that incorrect entries may not be corrected easily and sometimes the best might be to drop that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:56.827110Z",
     "start_time": "2020-08-06T14:37:56.482868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use kernel density estimation to color the density\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# We will reuse this plotting function later\n",
    "def plot_bc_numericals(_df):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 2.5), sharey=True)\n",
    "    y = df['recurrence'].astype('category').cat.codes.ravel()\n",
    "    xy = np.vstack([_df['age'],y]); z = gaussian_kde(xy)(xy)\n",
    "    axs[0].scatter(_df['age'], _df['recurrence'], c=z, s=50, edgecolor=None)\n",
    "    axs[0].set_xlabel('age')\n",
    "    xy = np.vstack([_df['tumor-size'],y]); z = gaussian_kde(xy)(xy)\n",
    "    axs[1].scatter(_df['tumor-size'], _df['recurrence'], c=z, s=50, edgecolor=None)\n",
    "    axs[1].set_xlabel('tumor-size')\n",
    "    xy = np.vstack([_df['inv-nodes'],y]); z = gaussian_kde(xy)(xy)\n",
    "    axs[2].scatter(_df['inv-nodes'], _df['recurrence'], c=z, s=50, edgecolor=None)\n",
    "    axs[2].set_xlabel('inv-nodes')\n",
    "    xy = np.vstack([_df['deg-malig'],y]); z = gaussian_kde(xy)(xy)\n",
    "    axs[3].scatter(_df['deg-malig'], _df['recurrence'], c=z, s=50, edgecolor=None)\n",
    "    axs[3].set_xlabel('deg-malig')\n",
    "    fig.suptitle('Breast-cancer dataset numerical variables')\n",
    "    plt.show()\n",
    "\n",
    "plot_bc_numericals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.140138Z",
     "start_time": "2020-08-06T14:37:56.828685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove that line with the incorrect age=250 and age=-5\n",
    "display(df[df['age']==250])\n",
    "index_to_drop = df[df['age']==250].index\n",
    "df.drop(index_to_drop, inplace=True)\n",
    "index_to_drop = df[df['age']==-5].index\n",
    "df.drop(index_to_drop, inplace=True)\n",
    "\n",
    "# Check results\n",
    "print(f'#total= {len(df)}')\n",
    "plot_bc_numericals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.146066Z",
     "start_time": "2020-08-06T14:37:57.142281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's reset the indices to the dataframe after dropping a few rows\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:45:22.855903Z",
     "start_time": "2020-08-06T13:45:22.853554Z"
    }
   },
   "source": [
    "## Cleaning complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:45:32.847206Z",
     "start_time": "2020-08-06T13:45:32.842235Z"
    }
   },
   "source": [
    "Compare the previous two cells to see the effect of removing the incorrect age entry.\n",
    "\n",
    "At this point we are ready to apply some learners such as the Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization is the process where a numerical variable is mapped to some levels by binning. This step is a big research/engineering area in machine learning. Recall that an example was provided in the past modules where the target (dependent) variable was discretized into three levels.\n",
    "\n",
    "For our purposes, in this step, we will do the post-discretization, and apply one hot encoding to a nominal/discretized variable. Note that the variable might be a nominal variable naturally, such as the 'breast' variable which takes values from the alphabet {'left', 'right'}.\n",
    "\n",
    "Now, we would like to continue preparing (preprocess) the dataset further to meet the requirements of the classifier that we would like to use - Random Forest classifier from scikit-learn library. This classifier works only on numerical data, thus we will convert the nominal variables into one hot encoded numerical variables, as explained in previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.154138Z",
     "start_time": "2020-08-06T14:37:57.147921Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas get_dummies function is the one-hot-encoder\n",
    "def encode_onehot(_df, f):\n",
    "    _df2 = pd.get_dummies(_df[f], prefix='', prefix_sep='').max(level=0, axis=1).add_prefix(f+' - ')\n",
    "    df3 = pd.concat([_df, _df2], axis=1)\n",
    "    df3 = df3.drop([f], axis=1)\n",
    "    return df3\n",
    "\n",
    "# Print nominal variables\n",
    "for f in list(df.columns.values):\n",
    "    if df[f].dtype == np.object:\n",
    "        print(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Will we one-hot-encode the variable 'recurrence'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.168300Z",
     "start_time": "2020-08-06T14:37:57.155786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the original\n",
    "display(df['menopause'][:10])\n",
    "\n",
    "# Apply the onehot-encoding method\n",
    "df_o = encode_onehot(df, 'menopause')\n",
    "\n",
    "# Check the onehot-encoded version of this feature\n",
    "cols = []\n",
    "for f in list(df_o.columns.values):\n",
    "    if 'menopause' in f:\n",
    "        cols += [f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.177305Z",
     "start_time": "2020-08-06T14:37:57.170040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the onehot-encoded        \n",
    "display(df_o[cols][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.201057Z",
     "start_time": "2020-08-06T14:37:57.178640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the rest of the nominal features too\n",
    "df_o = encode_onehot(df_o, 'node-caps')\n",
    "df_o = encode_onehot(df_o, 'breast')\n",
    "df_o = encode_onehot(df_o, 'breast-quad')\n",
    "df_o = encode_onehot(df_o, 'irradiat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.208938Z",
     "start_time": "2020-08-06T14:37:57.206541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check how many features we have\n",
    "print(f'before={len(df.columns)}, after={len(df_o.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.225882Z",
     "start_time": "2020-08-06T14:37:57.211715Z"
    }
   },
   "outputs": [],
   "source": [
    "df_o.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's classify the preprocessed dataset using the following strategies:\n",
    "\n",
    "* 80% random test-train split\n",
    "* Leave-one-out\n",
    "* 10-fold cross validation\n",
    "* Stratified 10-fold cross validation\n",
    "\n",
    "Note that the target variable is binary, predicting when the cancer is recurred, or the cancer did not recur. Clearly this dataset has ground truth captured from the data source, or in other words, dataset is pre-labeled, or carry the ground truth. Thus we will employ **supervised learning**.\n",
    "\n",
    "**Important:** Do not forget to remove the target (predicted, dependent) variable from X. Remember the Dataframe we are working already has the target variable and we will move it to y vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.233288Z",
     "start_time": "2020-08-06T14:37:57.227416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show that the dependent variable is unbalanced\n",
    "display(df['recurrence'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.490648Z",
     "start_time": "2020-08-06T14:37:57.236105Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "# We will reuse the classifier function below\n",
    "def rf_train_test(_X_tr, _X_ts, _y_tr, _y_ts):\n",
    "    # Create a new random forest classifier, with working 4 parallel cores\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=None, n_jobs=4)\n",
    "    # Train on training data\n",
    "    model = rf.fit(_X_tr, _y_tr)\n",
    "    # Test on training data\n",
    "    y_pred = rf.predict(_X_ts)\n",
    "    # Return accuracy\n",
    "    return accuracy_score(_y_ts, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.496444Z",
     "start_time": "2020-08-06T14:37:57.492013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the input X matrix and target y vector\n",
    "X = df_o.loc[:, df_o.columns != 'recurrence'].values\n",
    "y = df_o.loc[:, df_o.columns == 'recurrence'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.500192Z",
     "start_time": "2020-08-06T14:37:57.497770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:37:57.895224Z",
     "start_time": "2020-08-06T14:37:57.501639Z"
    }
   },
   "outputs": [],
   "source": [
    "# 80% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
    "rf_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What will be the performance (accuracy) when we run the above cell again? Will you see any variations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:01.933161Z",
     "start_time": "2020-08-06T14:37:57.897248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run 10 times\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
    "    print(rf_train_test(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** As the training and testing partition changes, the performance follows respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:54:29.047957Z",
     "start_time": "2020-08-06T13:54:29.044381Z"
    }
   },
   "source": [
    "**Question:** How can we measure the performance so that we can be sure of reporting it right?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:38:40.664170Z",
     "start_time": "2020-08-06T14:38:01.934597Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run 100 times and collect statistics\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
    "    accuracies += [rf_train_test(X_train, X_test, y_train, y_test)]\n",
    "#\n",
    "print(f'80% train-test split accuracy is {np.mean(accuracies):.3f} {chr(177)}{np.std(accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Leave-one-out evaluation keeps a single data point and label for test and uses all except for the test vector for training. Then, the evaluation process repeats this for each of the remaining data points, having a total number of  ùëÅ  accuracies.\n",
    "\n",
    "The sklearn API says train and test require a 2D X and 1D y even when there is only one data point. Below code generates the test vectors properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:34.207287Z",
     "start_time": "2020-08-06T14:38:40.665528Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Leave one out testing - this takes relatively longer\n",
    "N = X.shape[0]\n",
    "accuracies = []\n",
    "for i in range (0,N):\n",
    "    # Keep the 2D vector for the single test data point X\n",
    "    X_test = X[i].reshape(1, -1)\n",
    "    X_train = np.delete(np.array(X, copy=True), i, axis=0)\n",
    "    # Keep the 1D vector for the single test label y\n",
    "    y_test = [y[i]]\n",
    "    y_train = np.delete(np.array(y, copy=True), i, axis=0)\n",
    "    accuracies += [rf_train_test(X_train, X_test, y_train, y_test)]\n",
    "#\n",
    "# Sanity\n",
    "print(f'Leave-one-out accuracy N= {N}, #accuracies= {len(accuracies)}')\n",
    "# Score\n",
    "print(f'Leave-one-out accuracy is {np.mean(accuracies):.3f} {chr(177)}{np.std(accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:38.115442Z",
     "start_time": "2020-08-06T14:40:34.209210Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10-fold cross validation\n",
    "accuracies = []\n",
    "kf = KFold(n_splits=10,shuffle=False,random_state=None)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    acc = rf_train_test(X[train_index], X[test_index], y[train_index], y[test_index])\n",
    "    accuracies += [acc]\n",
    "#\n",
    "print(f'10-fold cross validation accuracy is {np.mean(accuracies):.3f} {chr(177)}{np.std(accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:42.025713Z",
     "start_time": "2020-08-06T14:40:38.117276Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def eval_classifier(X, y, niter):\n",
    "    accuracies = []\n",
    "    kf = StratifiedKFold(n_splits=10,shuffle=False,random_state=None)\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        acc = rf_train_test(X[train_index], X[test_index], y[train_index], y[test_index])\n",
    "        accuracies += [acc]\n",
    "    print( (f'Stratified 10-fold cross validation accuracy is '\n",
    "            f'{np.mean(accuracies):.3f} {chr(177)}{np.std(accuracies):.4f} with {niter} total iterations')\n",
    "         )\n",
    "#\n",
    "eval_classifier(X, y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the above performance results for discussion in the following cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:42.032259Z",
     "start_time": "2020-08-06T14:40:42.027506Z"
    }
   },
   "outputs": [],
   "source": [
    "**Question:** What are the differences between these four evaluation methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Question:** What are the differences between these four evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the differences between these four evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T13:59:35.401817Z",
     "start_time": "2020-08-06T13:59:35.399019Z"
    }
   },
   "source": [
    "Now that we preprocessed and used the data for classification we can move to other interesting problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, we did not have the ground truth, so that a supervised learning was not possible. A natural approach in this case is clustering the data to see if there are some patterns or models we can come up with that explains the cancer behavior. We will attempt answering questions like \"Is there a direct relation between menopause and cancer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's draw some plots where the x, y and z-dimensions are 'age', 'tumor-size', 'inv-nodes' and color is 'recurrence'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:42.162349Z",
     "start_time": "2020-08-06T14:40:42.033963Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Deep copy original dataframe\n",
    "df2 = df.copy()\n",
    "\n",
    "# Convert every feature to numbers\n",
    "df2['recurrence'] = df['recurrence'].astype(\"category\").cat.codes\n",
    "\n",
    "df2['menopause'] = df['menopause'].astype(\"category\").cat.codes.astype('float')\n",
    "df2['node-caps'] = df['node-caps'].astype(\"category\").cat.codes.astype('float')\n",
    "df2['breast'] = df['breast'].astype(\"category\").cat.codes.astype('float')\n",
    "df2['breast-quad'] = df['breast-quad'].astype(\"category\").cat.codes.astype('float')\n",
    "df2['irradiat'] = df['irradiat'].astype(\"category\").cat.codes.astype('float')\n",
    "\n",
    "df2['deg-malig'] = df['deg-malig'].astype('float')\n",
    "\n",
    "def draw3d(df, _mn, _mx):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlim3d(_mn, _mx)\n",
    "    ax.set_ylim3d(_mn, _mx)\n",
    "    ax.set_zlim3d(_mn, _mx)\n",
    "    ax.set_ylim(ax.get_ylim()[::-1]) \n",
    "    ax.scatter(df['age'], df['tumor-size'], df['inv-nodes'], c=df['recurrence'], s=30)\n",
    "    ax.set_xlabel('age'); ax.set_ylabel('tumor-size'); ax.set_zlabel('inv-nodes')\n",
    "#\n",
    "draw3d(df2, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Do the dimensions 'age', 'tumor-size', 'inv-nodes' look fine in the above 3D plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The features are clumped and not nicely occupy  [0‚àí100]  range, i.e. we are not seeing a spherical cluster shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cluster the cancer data, without using the ground truth. We have to convert the nominal variables to numerical by using the category codes like we applied to 'recurrence' variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:00:48.026109Z",
     "start_time": "2020-08-06T14:00:48.023306Z"
    }
   },
   "source": [
    "Important: Make sure every variable is of the same type, e.g. float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Note that the values 'recurrence' took {0,1}, and by looking at the 3d plot above, can we easily find out which values (0 or 1) corresponds to 'recurrence-events' levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:44.891763Z",
     "start_time": "2020-08-06T14:40:42.164114Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans(_X, _y, niter):  # do it niter times to collect statistics\n",
    "    accuracies = []\n",
    "    for i in range(niter):\n",
    "        # We know that there are two levels in tagret variable - thus n_clusters=2\n",
    "        km = KMeans(n_clusters=2, random_state=None)\n",
    "        clusters = km.fit_predict(_X)\n",
    "        accuracies += [accuracy_score(_y, clusters)]\n",
    "    #\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "X = df2.loc[:, df2.columns != 'recurrence'].values\n",
    "y = df2.loc[:, df2.columns == 'recurrence'].values.ravel()\n",
    "\n",
    "print(f'Clustering error= {kmeans(X, y, 100):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above performance is not very good as the error is almost equivalent to random choice, which would be  12  since we have 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the values of a column to  [0,1]  range is normalization:  ùë•ùëñ‚àímin(ùë•)max(ùë•)‚àímin(ùë•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is mapping the values to a  0 -mean  1 -standard-deviation distribution:  ùë•ùëñ‚àímean(ùë•)stdev(ùë•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization makes the optimization surface more spherical, which helps the optimizer using each feature with equal importance. This is especially important and helping for distance based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try two scalers from sklearn.preprocessing 1.Normalization MinMaxScaler(), 2. Standardization scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:45.025653Z",
     "start_time": "2020-08-06T14:40:44.894301Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df2[['age', 'tumor-size', 'inv-nodes']] = min_max_scaler.fit_transform(df2[['age', 'tumor-size', 'inv-nodes']])\n",
    "\n",
    "draw3d(df2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By normalizing the values through expansion and contraction to  [0,1]  we achieve the distance between the data points are in the same \"range\" or unit. Thus the distance metrics like Euclidean distance will weigh each dimension or feature equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Imagine a dataset which has speed in miles  [0,100]  and time traveled in seconds  [0,43200]  (12 hours max). A proper approach would be mapping both features into  [0,1]  scale to treat the feature space spherically. For actual feature values an inverse transformation can be used to map back to the original units (for example to be presented to user)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distance metric  ùëë  in  ùëÄ  dimensions (Dataframe has M number of columns) such as Euclidean  ùëëùëñùëò=‚àëùëÄùëó=0(ùë•ùëñùëó‚àíùë•ùëòùëó)2‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚àö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, clustering algorithms use some form of distance metric such as Euclidean distance between pairs of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above example, normalization of variables is a necessary step for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:46.667879Z",
     "start_time": "2020-08-06T14:40:45.027089Z"
    }
   },
   "outputs": [],
   "source": [
    "df2[['deg-malig', 'breast-quad']] = min_max_scaler.fit_transform(df2[['deg-malig', 'breast-quad']])\n",
    "\n",
    "X = df2.loc[:, df2.columns != 'recurrence'].values\n",
    "y = df2.loc[:, df2.columns == 'recurrence'].values.ravel()\n",
    "\n",
    "print(f'Clustering error= {kmeans(X, y, 100):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now standardization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:46.790511Z",
     "start_time": "2020-08-06T14:40:46.669469Z"
    }
   },
   "outputs": [],
   "source": [
    "df2[['age', 'tumor-size', 'inv-nodes']] = preprocessing.scale(df2[['age', 'tumor-size', 'inv-nodes']])\n",
    "\n",
    "draw3d(df2, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:46.805329Z",
     "start_time": "2020-08-06T14:40:46.792749Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:49.283427Z",
     "start_time": "2020-08-06T14:40:46.806802Z"
    }
   },
   "outputs": [],
   "source": [
    "df2[['deg-malig', 'breast-quad']] = preprocessing.scale(df2[['deg-malig', 'breast-quad']])\n",
    "\n",
    "X = df2.loc[:, df2.columns != 'recurrence'].values\n",
    "y = df2.loc[:, df2.columns == 'recurrence'].values.ravel()\n",
    "\n",
    "print(f'Clustering error= {kmeans(X, y, 100):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:49.811843Z",
     "start_time": "2020-08-06T14:40:49.284789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaled\n",
    "plot_bc(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:08:54.865947Z",
     "start_time": "2020-08-06T14:08:54.861837Z"
    }
   },
   "source": [
    "**Question:** Do you see any difference/improvement on the variables compared to the first set of plots in cell 1, repeated below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Shapes are same but axis scales are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:50.340785Z",
     "start_time": "2020-08-06T14:40:49.813285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "plot_bc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after variable transformation, variables become more spherical or Gaussian like, but then the levels or data points do not correspond to any meaningful value in the domain knowledge that the dataset originally belonged to. For example 'deg-malig' had three levels {1, 2, 3} which probably meant something to the doctors dealing with cancer patients. However depending on the dataset, such transformations make a difference, albeit a few percentage improvement on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "Reducing the data helps in a few ways:\n",
    "\n",
    "* Faster method run-time, such as training\n",
    "* More generalized models, decreases overfitting\n",
    "* Simpler models that make more sense to the domain expert or Subject Matter Expert (SME)\n",
    "* In some cases better accuracy performance - not necessarily always happens\n",
    "\n",
    "**Feature ranking** and **feature selection** is a common stage that is executed after cleaning and preprocessing the data. In the following cells we will examine the variable rankings by **Univariate Feature Selection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:50.491197Z",
     "start_time": "2020-08-06T14:40:50.342583Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "# Fit the data\n",
    "selector.fit(X, y)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "\n",
    "# Display\n",
    "cols = list(df2.loc[:, df2.columns != 'recurrence'].columns.values)\n",
    "y_pos = np.arange(len(cols))\n",
    "plt.bar(y_pos, scores)\n",
    "plt.xticks(y_pos, cols, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Can we drop 'age', 'menopause', 'breast', 'breast-quad' variables and redo the classification evaluation without a performance loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:50.501096Z",
     "start_time": "2020-08-06T14:40:50.492760Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.drop(columns='age', inplace=True)\n",
    "df3.drop(columns='menopause', inplace=True)\n",
    "df3.drop(columns='breast', inplace=True)\n",
    "df3.drop(columns='breast-quad', inplace=True)\n",
    "\n",
    "X = df3.loc[:, df3.columns != 'recurrence'].values\n",
    "y = df3.loc[:, df3.columns == 'recurrence'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:54.461002Z",
     "start_time": "2020-08-06T14:40:50.503279Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_classifier(X, y, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The performance accuracy did not drop. And we have less data columns now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we had standardized the data in the previous steps. Let's go back to the original dataset just after the cleaning was completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:54.479426Z",
     "start_time": "2020-08-06T14:40:54.462906Z"
    }
   },
   "outputs": [],
   "source": [
    "df4 = df_o.copy()\n",
    "df4.drop(columns='age', inplace=True)\n",
    "\n",
    "# 'menopause' was onehot-encoded\n",
    "for col in df4.columns.values:\n",
    "    if 'menopause' in col:\n",
    "        df4.drop(columns=col, inplace=True)\n",
    "\n",
    "# 'breast' was onehot-encoded\n",
    "for col in df4.columns.values:\n",
    "    if 'breast' in col:\n",
    "        df4.drop(columns=col, inplace=True)\n",
    "\n",
    "# 'breast-quad' was onehot-encoded\n",
    "for col in df4.columns.values:\n",
    "    if 'breast-quad' in col:\n",
    "        df4.drop(columns=col, inplace=True)\n",
    "\n",
    "X = df4.loc[:, df4.columns != 'recurrence'].values\n",
    "y = df4.loc[:, df4.columns == 'recurrence'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T14:40:58.410070Z",
     "start_time": "2020-08-06T14:40:54.481199Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_classifier(X, y, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More success! The performance accuracy increased! Or did we bias it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder Question: Do you accept the performance increase as a valid increase? Or would you attribute it to the variance of error?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the most important take-away in this effort?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
